{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-31T15:02:58.550534Z","iopub.execute_input":"2022-08-31T15:02:58.551002Z","iopub.status.idle":"2022-08-31T15:02:58.562437Z","shell.execute_reply.started":"2022-08-31T15:02:58.550950Z","shell.execute_reply":"2022-08-31T15:02:58.561140Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Homework description\n\nImplement simple mixture of experts:\n\n- ROUTER: clusterize your data (on features). Create 2 clusters.\n- EXPERTS: Build experts (GB ensamble M=10) per clusters\n- AGGREGATOR: you should define it. You can choose: Argmax (probabilities), wieghted sum, NN over expert's outputs and probabilities","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:13.964874Z","iopub.execute_input":"2022-08-31T17:06:13.965332Z","iopub.status.idle":"2022-08-31T17:06:14.017527Z","shell.execute_reply.started":"2022-08-31T17:06:13.965293Z","shell.execute_reply":"2022-08-31T17:06:14.016494Z"},"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\nx_train = train_data[numeric_columns].fillna(-1)\nx_test = test_data[numeric_columns].fillna(-1)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:15.222240Z","iopub.execute_input":"2022-08-31T17:06:15.222721Z","iopub.status.idle":"2022-08-31T17:06:15.237507Z","shell.execute_reply.started":"2022-08-31T17:06:15.222671Z","shell.execute_reply":"2022-08-31T17:06:15.235514Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"y_train = np.log(train_data['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:17.327304Z","iopub.execute_input":"2022-08-31T17:06:17.328425Z","iopub.status.idle":"2022-08-31T17:06:17.334764Z","shell.execute_reply.started":"2022-08-31T17:06:17.328380Z","shell.execute_reply":"2022-08-31T17:06:17.333275Z"},"trusted":true},"execution_count":79,"outputs":[]},{"cell_type":"code","source":"def rmse(a, b):\n    return ((a - b) ** 2).mean() ** 0.5","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:19.378500Z","iopub.execute_input":"2022-08-31T17:06:19.378915Z","iopub.status.idle":"2022-08-31T17:06:19.384498Z","shell.execute_reply.started":"2022-08-31T17:06:19.378879Z","shell.execute_reply":"2022-08-31T17:06:19.383363Z"},"trusted":true},"execution_count":80,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import KMeans\n\nN_CLUSTERS = 2\nKMEANS_RANDOM_STATE = 0\n\nkmeans = KMeans(n_clusters=N_CLUSTERS, random_state=KMEANS_RANDOM_STATE).fit(x_train)\nnp.unique(kmeans.labels_, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:24.114699Z","iopub.execute_input":"2022-08-31T17:06:24.115151Z","iopub.status.idle":"2022-08-31T17:06:25.084332Z","shell.execute_reply.started":"2022-08-31T17:06:24.115112Z","shell.execute_reply":"2022-08-31T17:06:25.083229Z"},"trusted":true},"execution_count":81,"outputs":[{"execution_count":81,"output_type":"execute_result","data":{"text/plain":"(array([0, 1], dtype=int32), array([   4, 1456]))"},"metadata":{}}]},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\nstandard_scaler = StandardScaler().fit(x_train)\nx_train = standard_scaler.transform(x_train)\nx_test = standard_scaler.transform(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:28.217353Z","iopub.execute_input":"2022-08-31T17:06:28.218302Z","iopub.status.idle":"2022-08-31T17:06:28.234009Z","shell.execute_reply.started":"2022-08-31T17:06:28.218257Z","shell.execute_reply":"2022-08-31T17:06:28.232790Z"},"trusted":true},"execution_count":82,"outputs":[]},{"cell_type":"code","source":"kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=KMEANS_RANDOM_STATE).fit(x_train)\nnp.unique(kmeans.labels_, return_counts=True)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:30.248270Z","iopub.execute_input":"2022-08-31T17:06:30.248676Z","iopub.status.idle":"2022-08-31T17:06:31.581527Z","shell.execute_reply.started":"2022-08-31T17:06:30.248641Z","shell.execute_reply":"2022-08-31T17:06:31.580517Z"},"trusted":true},"execution_count":83,"outputs":[{"execution_count":83,"output_type":"execute_result","data":{"text/plain":"(array([0, 1], dtype=int32), array([714, 746]))"},"metadata":{}}]},{"cell_type":"code","source":"x_train = pd.DataFrame(x_train, columns=numeric_columns)\nx_test = pd.DataFrame(x_test, columns=numeric_columns)\nx_train","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:34.306563Z","iopub.execute_input":"2022-08-31T17:06:34.307052Z","iopub.status.idle":"2022-08-31T17:06:34.340547Z","shell.execute_reply.started":"2022-08-31T17:06:34.307000Z","shell.execute_reply":"2022-08-31T17:06:34.339262Z"},"trusted":true},"execution_count":84,"outputs":[{"execution_count":84,"output_type":"execute_result","data":{"text/plain":"      MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n0       0.073375     0.216151 -0.207142     0.651479    -0.517200   1.050994   \n1      -0.872563     0.645357 -0.091886    -0.071836     2.179628   0.156734   \n2       0.073375     0.301992  0.073480     0.651479    -0.517200   0.984752   \n3       0.309859     0.073083 -0.096897     0.651479    -0.517200  -1.863632   \n4       0.073375     0.759812  0.375148     1.374795    -0.517200   0.951632   \n...          ...          ...       ...          ...          ...        ...   \n1455    0.073375     0.130310 -0.260560    -0.071836    -0.517200   0.918511   \n1456   -0.872563     0.788426  0.266407    -0.071836     0.381743   0.222975   \n1457    0.309859     0.244765 -0.147810     0.651479     3.078570  -1.002492   \n1458   -0.872563     0.301992 -0.080160    -0.795151     0.381743  -0.704406   \n1459   -0.872563     0.502288 -0.058112    -0.795151     0.381743  -0.207594   \n\n      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  GarageArea  \\\n0         0.878668    0.514125    0.575425   -0.288653  ...    0.351000   \n1        -0.429577   -0.570710    1.171992   -0.288653  ...   -0.060731   \n2         0.830215    0.325940    0.092907   -0.288653  ...    0.631726   \n3        -0.720298   -0.570710   -0.499274   -0.288653  ...    0.790804   \n4         0.733308    1.366496    0.463568   -0.288653  ...    1.698485   \n...            ...         ...         ...         ...  ...         ...   \n1455      0.733308   -0.570710   -0.973018   -0.288653  ...   -0.060731   \n1456      0.151865    0.087940    0.759659    0.722112  ...    0.126420   \n1457      1.024029   -0.570710   -0.369871   -0.288653  ...   -1.033914   \n1458      0.539493   -0.570710   -0.865548    6.092188  ...   -1.090059   \n1459     -0.962566   -0.570710    0.847389    1.509640  ...   -0.921624   \n\n      WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  \\\n0      -0.752176     0.216503      -0.359325  -0.116339    -0.270208   \n1       1.626195    -0.704483      -0.359325  -0.116339    -0.270208   \n2      -0.752176    -0.070361      -0.359325  -0.116339    -0.270208   \n3      -0.752176    -0.176048       4.092524  -0.116339    -0.270208   \n4       0.780197     0.563760      -0.359325  -0.116339    -0.270208   \n...          ...          ...            ...        ...          ...   \n1455   -0.752176    -0.100558      -0.359325  -0.116339    -0.270208   \n1456    2.033231    -0.704483      -0.359325  -0.116339    -0.270208   \n1457   -0.752176     0.201405      -0.359325  -0.116339    -0.270208   \n1458    2.168910    -0.704483       1.473789  -0.116339    -0.270208   \n1459    5.121921     0.322190      -0.359325  -0.116339    -0.270208   \n\n      PoolArea   MiscVal    MoSold    YrSold  \n0    -0.068692 -0.087688 -1.599111  0.138777  \n1    -0.068692 -0.087688 -0.489110 -0.614439  \n2    -0.068692 -0.087688  0.990891  0.138777  \n3    -0.068692 -0.087688 -1.599111 -1.367655  \n4    -0.068692 -0.087688  2.100892  0.138777  \n...        ...       ...       ...       ...  \n1455 -0.068692 -0.087688  0.620891 -0.614439  \n1456 -0.068692 -0.087688 -1.599111  1.645210  \n1457 -0.068692  4.953112 -0.489110  1.645210  \n1458 -0.068692 -0.087688 -0.859110  1.645210  \n1459 -0.068692 -0.087688 -0.119110  0.138777  \n\n[1460 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.073375</td>\n      <td>0.216151</td>\n      <td>-0.207142</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>1.050994</td>\n      <td>0.878668</td>\n      <td>0.514125</td>\n      <td>0.575425</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>0.351000</td>\n      <td>-0.752176</td>\n      <td>0.216503</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>0.138777</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.872563</td>\n      <td>0.645357</td>\n      <td>-0.091886</td>\n      <td>-0.071836</td>\n      <td>2.179628</td>\n      <td>0.156734</td>\n      <td>-0.429577</td>\n      <td>-0.570710</td>\n      <td>1.171992</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.060731</td>\n      <td>1.626195</td>\n      <td>-0.704483</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.489110</td>\n      <td>-0.614439</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.073375</td>\n      <td>0.301992</td>\n      <td>0.073480</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>0.984752</td>\n      <td>0.830215</td>\n      <td>0.325940</td>\n      <td>0.092907</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>0.631726</td>\n      <td>-0.752176</td>\n      <td>-0.070361</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>0.990891</td>\n      <td>0.138777</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.309859</td>\n      <td>0.073083</td>\n      <td>-0.096897</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>-1.863632</td>\n      <td>-0.720298</td>\n      <td>-0.570710</td>\n      <td>-0.499274</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>0.790804</td>\n      <td>-0.752176</td>\n      <td>-0.176048</td>\n      <td>4.092524</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>-1.367655</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.073375</td>\n      <td>0.759812</td>\n      <td>0.375148</td>\n      <td>1.374795</td>\n      <td>-0.517200</td>\n      <td>0.951632</td>\n      <td>0.733308</td>\n      <td>1.366496</td>\n      <td>0.463568</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>1.698485</td>\n      <td>0.780197</td>\n      <td>0.563760</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>2.100892</td>\n      <td>0.138777</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>0.073375</td>\n      <td>0.130310</td>\n      <td>-0.260560</td>\n      <td>-0.071836</td>\n      <td>-0.517200</td>\n      <td>0.918511</td>\n      <td>0.733308</td>\n      <td>-0.570710</td>\n      <td>-0.973018</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.060731</td>\n      <td>-0.752176</td>\n      <td>-0.100558</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>0.620891</td>\n      <td>-0.614439</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>-0.872563</td>\n      <td>0.788426</td>\n      <td>0.266407</td>\n      <td>-0.071836</td>\n      <td>0.381743</td>\n      <td>0.222975</td>\n      <td>0.151865</td>\n      <td>0.087940</td>\n      <td>0.759659</td>\n      <td>0.722112</td>\n      <td>...</td>\n      <td>0.126420</td>\n      <td>2.033231</td>\n      <td>-0.704483</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>1.645210</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>0.309859</td>\n      <td>0.244765</td>\n      <td>-0.147810</td>\n      <td>0.651479</td>\n      <td>3.078570</td>\n      <td>-1.002492</td>\n      <td>1.024029</td>\n      <td>-0.570710</td>\n      <td>-0.369871</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-1.033914</td>\n      <td>-0.752176</td>\n      <td>0.201405</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>4.953112</td>\n      <td>-0.489110</td>\n      <td>1.645210</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>-0.872563</td>\n      <td>0.301992</td>\n      <td>-0.080160</td>\n      <td>-0.795151</td>\n      <td>0.381743</td>\n      <td>-0.704406</td>\n      <td>0.539493</td>\n      <td>-0.570710</td>\n      <td>-0.865548</td>\n      <td>6.092188</td>\n      <td>...</td>\n      <td>-1.090059</td>\n      <td>2.168910</td>\n      <td>-0.704483</td>\n      <td>1.473789</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.859110</td>\n      <td>1.645210</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>-0.872563</td>\n      <td>0.502288</td>\n      <td>-0.058112</td>\n      <td>-0.795151</td>\n      <td>0.381743</td>\n      <td>-0.207594</td>\n      <td>-0.962566</td>\n      <td>-0.570710</td>\n      <td>0.847389</td>\n      <td>1.509640</td>\n      <td>...</td>\n      <td>-0.921624</td>\n      <td>5.121921</td>\n      <td>0.322190</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.119110</td>\n      <td>0.138777</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x_train['Cluster'] = kmeans.labels_\nx_train","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:06:38.435164Z","iopub.execute_input":"2022-08-31T17:06:38.435824Z","iopub.status.idle":"2022-08-31T17:06:38.473593Z","shell.execute_reply.started":"2022-08-31T17:06:38.435771Z","shell.execute_reply":"2022-08-31T17:06:38.472368Z"},"trusted":true},"execution_count":85,"outputs":[{"execution_count":85,"output_type":"execute_result","data":{"text/plain":"      MSSubClass  LotFrontage   LotArea  OverallQual  OverallCond  YearBuilt  \\\n0       0.073375     0.216151 -0.207142     0.651479    -0.517200   1.050994   \n1      -0.872563     0.645357 -0.091886    -0.071836     2.179628   0.156734   \n2       0.073375     0.301992  0.073480     0.651479    -0.517200   0.984752   \n3       0.309859     0.073083 -0.096897     0.651479    -0.517200  -1.863632   \n4       0.073375     0.759812  0.375148     1.374795    -0.517200   0.951632   \n...          ...          ...       ...          ...          ...        ...   \n1455    0.073375     0.130310 -0.260560    -0.071836    -0.517200   0.918511   \n1456   -0.872563     0.788426  0.266407    -0.071836     0.381743   0.222975   \n1457    0.309859     0.244765 -0.147810     0.651479     3.078570  -1.002492   \n1458   -0.872563     0.301992 -0.080160    -0.795151     0.381743  -0.704406   \n1459   -0.872563     0.502288 -0.058112    -0.795151     0.381743  -0.207594   \n\n      YearRemodAdd  MasVnrArea  BsmtFinSF1  BsmtFinSF2  ...  WoodDeckSF  \\\n0         0.878668    0.514125    0.575425   -0.288653  ...   -0.752176   \n1        -0.429577   -0.570710    1.171992   -0.288653  ...    1.626195   \n2         0.830215    0.325940    0.092907   -0.288653  ...   -0.752176   \n3        -0.720298   -0.570710   -0.499274   -0.288653  ...   -0.752176   \n4         0.733308    1.366496    0.463568   -0.288653  ...    0.780197   \n...            ...         ...         ...         ...  ...         ...   \n1455      0.733308   -0.570710   -0.973018   -0.288653  ...   -0.752176   \n1456      0.151865    0.087940    0.759659    0.722112  ...    2.033231   \n1457      1.024029   -0.570710   -0.369871   -0.288653  ...   -0.752176   \n1458      0.539493   -0.570710   -0.865548    6.092188  ...    2.168910   \n1459     -0.962566   -0.570710    0.847389    1.509640  ...    5.121921   \n\n      OpenPorchSF  EnclosedPorch  3SsnPorch  ScreenPorch  PoolArea   MiscVal  \\\n0        0.216503      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n1       -0.704483      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n2       -0.070361      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n3       -0.176048       4.092524  -0.116339    -0.270208 -0.068692 -0.087688   \n4        0.563760      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n...           ...            ...        ...          ...       ...       ...   \n1455    -0.100558      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n1456    -0.704483      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n1457     0.201405      -0.359325  -0.116339    -0.270208 -0.068692  4.953112   \n1458    -0.704483       1.473789  -0.116339    -0.270208 -0.068692 -0.087688   \n1459     0.322190      -0.359325  -0.116339    -0.270208 -0.068692 -0.087688   \n\n        MoSold    YrSold  Cluster  \n0    -1.599111  0.138777        0  \n1    -0.489110 -0.614439        1  \n2     0.990891  0.138777        0  \n3    -1.599111 -1.367655        1  \n4     2.100892  0.138777        0  \n...        ...       ...      ...  \n1455  0.620891 -0.614439        0  \n1456 -1.599111  1.645210        0  \n1457 -0.489110  1.645210        0  \n1458 -0.859110  1.645210        1  \n1459 -0.119110  0.138777        1  \n\n[1460 rows x 37 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n      <th>Cluster</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.073375</td>\n      <td>0.216151</td>\n      <td>-0.207142</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>1.050994</td>\n      <td>0.878668</td>\n      <td>0.514125</td>\n      <td>0.575425</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.752176</td>\n      <td>0.216503</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>0.138777</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>-0.872563</td>\n      <td>0.645357</td>\n      <td>-0.091886</td>\n      <td>-0.071836</td>\n      <td>2.179628</td>\n      <td>0.156734</td>\n      <td>-0.429577</td>\n      <td>-0.570710</td>\n      <td>1.171992</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>1.626195</td>\n      <td>-0.704483</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.489110</td>\n      <td>-0.614439</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.073375</td>\n      <td>0.301992</td>\n      <td>0.073480</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>0.984752</td>\n      <td>0.830215</td>\n      <td>0.325940</td>\n      <td>0.092907</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.752176</td>\n      <td>-0.070361</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>0.990891</td>\n      <td>0.138777</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.309859</td>\n      <td>0.073083</td>\n      <td>-0.096897</td>\n      <td>0.651479</td>\n      <td>-0.517200</td>\n      <td>-1.863632</td>\n      <td>-0.720298</td>\n      <td>-0.570710</td>\n      <td>-0.499274</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.752176</td>\n      <td>-0.176048</td>\n      <td>4.092524</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>-1.367655</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.073375</td>\n      <td>0.759812</td>\n      <td>0.375148</td>\n      <td>1.374795</td>\n      <td>-0.517200</td>\n      <td>0.951632</td>\n      <td>0.733308</td>\n      <td>1.366496</td>\n      <td>0.463568</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>0.780197</td>\n      <td>0.563760</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>2.100892</td>\n      <td>0.138777</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>0.073375</td>\n      <td>0.130310</td>\n      <td>-0.260560</td>\n      <td>-0.071836</td>\n      <td>-0.517200</td>\n      <td>0.918511</td>\n      <td>0.733308</td>\n      <td>-0.570710</td>\n      <td>-0.973018</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.752176</td>\n      <td>-0.100558</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>0.620891</td>\n      <td>-0.614439</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>-0.872563</td>\n      <td>0.788426</td>\n      <td>0.266407</td>\n      <td>-0.071836</td>\n      <td>0.381743</td>\n      <td>0.222975</td>\n      <td>0.151865</td>\n      <td>0.087940</td>\n      <td>0.759659</td>\n      <td>0.722112</td>\n      <td>...</td>\n      <td>2.033231</td>\n      <td>-0.704483</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-1.599111</td>\n      <td>1.645210</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>0.309859</td>\n      <td>0.244765</td>\n      <td>-0.147810</td>\n      <td>0.651479</td>\n      <td>3.078570</td>\n      <td>-1.002492</td>\n      <td>1.024029</td>\n      <td>-0.570710</td>\n      <td>-0.369871</td>\n      <td>-0.288653</td>\n      <td>...</td>\n      <td>-0.752176</td>\n      <td>0.201405</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>4.953112</td>\n      <td>-0.489110</td>\n      <td>1.645210</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>-0.872563</td>\n      <td>0.301992</td>\n      <td>-0.080160</td>\n      <td>-0.795151</td>\n      <td>0.381743</td>\n      <td>-0.704406</td>\n      <td>0.539493</td>\n      <td>-0.570710</td>\n      <td>-0.865548</td>\n      <td>6.092188</td>\n      <td>...</td>\n      <td>2.168910</td>\n      <td>-0.704483</td>\n      <td>1.473789</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.859110</td>\n      <td>1.645210</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1459</th>\n      <td>-0.872563</td>\n      <td>0.502288</td>\n      <td>-0.058112</td>\n      <td>-0.795151</td>\n      <td>0.381743</td>\n      <td>-0.207594</td>\n      <td>-0.962566</td>\n      <td>-0.570710</td>\n      <td>0.847389</td>\n      <td>1.509640</td>\n      <td>...</td>\n      <td>5.121921</td>\n      <td>0.322190</td>\n      <td>-0.359325</td>\n      <td>-0.116339</td>\n      <td>-0.270208</td>\n      <td>-0.068692</td>\n      <td>-0.087688</td>\n      <td>-0.119110</td>\n      <td>0.138777</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>1460 rows × 37 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"**Catboost only, avg ensemble**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor\nfrom tqdm import tqdm\n\nM = 10\n\nx_train_clustered_list = [x_train[x_train['Cluster'] == i].drop(columns=['Cluster']) for i in range(N_CLUSTERS)]\ny_train_clustered_list = [y_train[x_train[x_train['Cluster'] == i].index] for i in range(N_CLUSTERS)]\n\nexperts_rmses_tr = []\nexperts_rmses_val = []\nexperts_preds_test = []\n\nfor x_train_clustered, y_train_clustered in zip(x_train_clustered_list, y_train_clustered_list):\n    rmses_tr = []\n    rmses_val = []\n    y_preds_test = []\n    for m in tqdm(range(M)):\n        x_tr, x_val, y_tr, y_val = train_test_split(x_train_clustered, y_train_clustered, test_size=0.1, random_state=m)\n        model = CatBoostRegressor(verbose=False).fit(x_tr, y_tr)\n        y_pred_tr = model.predict(x_tr)\n        rmses_tr.append(rmse(y_tr, y_pred_tr))\n        y_pred_val = model.predict(x_val)\n        rmses_val.append(rmse(y_val, y_pred_val))\n        y_preds_test.append(model.predict(x_test))\n    experts_rmses_tr.append(np.exp(np.mean(rmses_tr, axis=0)))\n    experts_rmses_val.append(np.exp(np.mean(rmses_val, axis=0)))    \n    experts_preds_test.append(np.exp(np.mean(y_preds_test, axis=0)))    ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:08:54.487312Z","iopub.execute_input":"2022-08-31T17:08:54.487706Z","iopub.status.idle":"2022-08-31T17:09:39.627942Z","shell.execute_reply.started":"2022-08-31T17:08:54.487670Z","shell.execute_reply":"2022-08-31T17:09:39.626843Z"},"trusted":true},"execution_count":87,"outputs":[{"name":"stderr","text":"100%|██████████| 10/10 [00:23<00:00,  2.32s/it]\n100%|██████████| 10/10 [00:21<00:00,  2.19s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"experts_rmses_tr","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:09:44.030277Z","iopub.execute_input":"2022-08-31T17:09:44.030666Z","iopub.status.idle":"2022-08-31T17:09:44.038604Z","shell.execute_reply.started":"2022-08-31T17:09:44.030634Z","shell.execute_reply":"2022-08-31T17:09:44.037361Z"},"trusted":true},"execution_count":88,"outputs":[{"execution_count":88,"output_type":"execute_result","data":{"text/plain":"[1.0235643371732541, 1.030366035946671]"},"metadata":{}}]},{"cell_type":"code","source":"experts_rmses_val","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:09:46.428607Z","iopub.execute_input":"2022-08-31T17:09:46.429094Z","iopub.status.idle":"2022-08-31T17:09:46.436653Z","shell.execute_reply.started":"2022-08-31T17:09:46.429054Z","shell.execute_reply":"2022-08-31T17:09:46.435408Z"},"trusted":true},"execution_count":89,"outputs":[{"execution_count":89,"output_type":"execute_result","data":{"text/plain":"[1.1351729526581429, 1.1533165605504823]"},"metadata":{}}]},{"cell_type":"code","source":"avg_experts_preds_test = np.mean(experts_preds_test, axis=0)\navg_experts_preds_test","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:47:00.066233Z","iopub.execute_input":"2022-08-31T15:47:00.067042Z","iopub.status.idle":"2022-08-31T15:47:00.077028Z","shell.execute_reply.started":"2022-08-31T15:47:00.066966Z","shell.execute_reply":"2022-08-31T15:47:00.075498Z"},"trusted":true},"execution_count":57,"outputs":[{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"array([142979.13265101, 161598.46646019, 179356.06414367, ...,\n       191202.62641849, 133283.41434865, 211027.64328954])"},"metadata":{}}]},{"cell_type":"code","source":"submit_avg = pd.DataFrame()\nsubmit_avg['Id'] = test_data['Id']\nsubmit_avg['SalePrice'] = avg_experts_preds_test\n\nsubmit_avg.to_csv('/kaggle/working/mixture_of_experts_avg.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T15:47:58.283899Z","iopub.execute_input":"2022-08-31T15:47:58.284364Z","iopub.status.idle":"2022-08-31T15:47:58.300599Z","shell.execute_reply.started":"2022-08-31T15:47:58.284325Z","shell.execute_reply":"2022-08-31T15:47:58.299283Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":"**Different GB, wieghted sum ensemble**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\n# !pip install tensorflow_decision_forests\nimport tensorflow_decision_forests as tfdf\n\ngbs = [\n    ('sklearn', GradientBoostingRegressor(), 0.05),\n    ('sklearn', GradientBoostingRegressor(), 0.05),\n    ('lgb', LGBMRegressor(), 0.1),\n    ('lgb', LGBMRegressor(), 0.1),\n    ('xgb', XGBRegressor(), 0.1),\n    ('xgb', XGBRegressor(), 0.1),\n    ('keras', tfdf.keras.GradientBoostedTreesModel(task=tfdf.keras.Task.REGRESSION, verbose=0), 0.1),\n    ('keras', tfdf.keras.GradientBoostedTreesModel(task=tfdf.keras.Task.REGRESSION, verbose=0), 0.1),\n    ('catb', CatBoostRegressor(verbose=False), 0.15),\n    ('catb', CatBoostRegressor(verbose=False), 0.15),\n]\n\nexperts_rmses_tr = []\nexperts_rmses_val = []\nexperts_preds_test = []\n\nfor x_train_clustered, y_train_clustered in zip(x_train_clustered_list, y_train_clustered_list):\n    rmses_tr = []\n    rmses_val = []\n    y_preds_test = []\n    for m in tqdm(range(M)):\n        x_tr, x_val, y_tr, y_val = train_test_split(x_train_clustered, y_train_clustered, test_size=0.1, random_state=m)\n        if gbs[m][0] != 'keras':\n            model = gbs[m][1].fit(x_tr, y_tr)\n            \n            y_pred_tr = model.predict(x_tr)\n            y_pred_val = model.predict(x_val)\n            y_pred_test = model.predict(x_test)\n        else:\n            tr = pd.concat([x_tr, y_tr], axis=1)\n            tf_tr = tfdf.keras.pd_dataframe_to_tf_dataset(tr, label='SalePrice', task=tfdf.keras.Task.REGRESSION)\n            \n            model = tfdf.keras.GradientBoostedTreesModel(task=tfdf.keras.Task.REGRESSION, verbose=0)\n            model.fit(x=tf_tr, verbose=0)\n\n            y_pred_tr = model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(x_tr, task=tfdf.keras.Task.REGRESSION), verbose=0)\n            y_pred_val = model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(x_val, task=tfdf.keras.Task.REGRESSION), verbose=0)\n            y_pred_test = model.predict(tfdf.keras.pd_dataframe_to_tf_dataset(x_test, task=tfdf.keras.Task.REGRESSION), verbose=0)\n\n            y_pred_tr = np.array([pred_tr_el for pred_tr in y_pred_tr for pred_tr_el in pred_tr])\n            y_pred_val = np.array([pred_val_el for pred_val in y_pred_val for pred_val_el in pred_val])\n            y_pred_test = np.array([pred_test_el for pred_test in y_pred_test for pred_test_el in pred_test])\n                        \n        rmses_tr.append(rmse(y_tr, y_pred_tr))\n        rmses_val.append(rmse(y_val, y_pred_val))\n        y_preds_test.append(gbs[m][2] * y_pred_test)\n    \n    experts_rmses_tr.append(np.exp(np.mean(rmses_tr, axis=0)))\n    experts_rmses_val.append(np.exp(np.mean(rmses_val, axis=0)))    \n    experts_preds_test.append(np.exp(np.sum(y_preds_test, axis=0)))    ","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:14:09.373892Z","iopub.execute_input":"2022-08-31T17:14:09.374387Z","iopub.status.idle":"2022-08-31T17:14:32.669652Z","shell.execute_reply.started":"2022-08-31T17:14:09.374340Z","shell.execute_reply":"2022-08-31T17:14:32.668412Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stderr","text":" 60%|██████    | 6/10 [00:01<00:01,  3.00it/s]/opt/conda/lib/python3.7/site-packages/tensorflow_decision_forests/keras/core.py:1612: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n  features_dataframe = dataframe.drop(label, 1)\n[INFO kernel.cc:736] Start Yggdrasil model training\n[INFO kernel.cc:737] Collect training examples\n[INFO kernel.cc:392] Number of batches: 11\n[INFO kernel.cc:393] Number of examples: 642\n[INFO kernel.cc:759] Dataset:\nNumber of records: 642\nNumber of columns: 37\n\nNumber of columns by type:\n\tNUMERICAL: 37 (100%)\n\nColumns:\n\nNUMERICAL: 37 (100%)\n\t0: \"1stFlrSF\" NUMERICAL mean:0.479986 min:-1.72757 max:9.13268 sd:1.11765\n\t1: \"2ndFlrSF\" NUMERICAL mean:0.348767 min:-0.795163 max:3.93696 sd:1.14877\n\t2: \"3SsnPorch\" NUMERICAL mean:0.0648964 min:-0.116339 max:17.2172 sd:1.31002\n\t3: \"BedroomAbvGr\" NUMERICAL mean:0.21726 min:-3.51495 max:6.295 sd:0.997449\n\t4: \"BsmtFinSF1\" NUMERICAL mean:0.177092 min:-0.973018 max:11.4058 sd:1.21391\n\t5: \"BsmtFinSF2\" NUMERICAL mean:-0.0618716 min:-0.288653 max:8.85164 sd:0.984408\n\t6: \"BsmtFullBath\" NUMERICAL mean:0.065851 min:-0.819964 max:3.03558 sd:1.00777\n\t7: \"BsmtHalfBath\" NUMERICAL mean:-0.0583253 min:-0.241061 max:3.94881 sd:0.855714\n\t8: \"BsmtUnfSF\" NUMERICAL mean:0.324392 min:-1.28418 max:4.00429 sd:1.13362\n\t9: \"EnclosedPorch\" NUMERICAL mean:-0.174494 min:-0.359325 max:8.67531 sd:0.831032\n\t10: \"Fireplaces\" NUMERICAL mean:0.399883 min:-0.951226 max:3.70394 sd:0.961062\n\t11: \"FullBath\" NUMERICAL mean:0.755801 min:-2.84182 max:2.60552 sd:0.623822\n\t12: \"GarageArea\" NUMERICAL mean:0.54624 min:-2.21296 max:4.42153 sd:0.802915\n\t13: \"GarageCars\" NUMERICAL mean:0.616137 min:-2.36544 max:2.98889 sd:0.648746\n\t14: \"GarageYrBlt\" NUMERICAL mean:0.259716 min:-4.12033 max:0.311426 sd:0.247987\n\t15: \"GrLivArea\" NUMERICAL mean:0.640554 min:-0.796618 max:7.85557 sd:0.961118\n\t16: \"HalfBath\" NUMERICAL mean:0.29805 min:-0.761621 max:3.21679 sd:1.02906\n\t17: \"KitchenAbvGr\" NUMERICAL mean:-0.0488044 min:-0.211454 max:4.32858 sd:0.843788\n\t18: \"LotArea\" NUMERICAL mean:0.184062 min:-0.841848 max:20.5183 sd:1.3912\n\t19: \"LotFrontage\" NUMERICAL mean:0.0843587 min:-1.67235 max:7.31235 sd:1.10765\n\t20: \"LowQualFinSF\" NUMERICAL mean:-0.0247773 min:-0.120242 max:11.6477 sd:0.920771\n\t21: \"MSSubClass\" NUMERICAL mean:0.0407755 min:-0.872563 max:3.14767 sd:0.907062\n\t22: \"MasVnrArea\" NUMERICAL mean:0.35351 min:-0.576245 max:8.28509 sd:1.23522\n\t23: \"MiscVal\" NUMERICAL mean:-0.0221103 min:-0.0876878 max:16.6478 sd:0.739458\n\t24: \"MoSold\" NUMERICAL mean:0.0474481 min:-1.96911 max:2.10089 sd:1.03204\n\t25: \"OpenPorchSF\" NUMERICAL mean:0.357748 min:-0.704483 max:7.5542 sd:1.07701\n\t26: \"OverallCond\" NUMERICAL mean:-0.252558 min:-3.21403 max:3.07857 sd:0.770266\n\t27: \"OverallQual\" NUMERICAL mean:0.689786 min:-1.51847 max:2.82143 sd:0.785016\n\t28: \"PoolArea\" NUMERICAL mean:0.0400537 min:-0.0686917 max:18.3062 sd:1.24387\n\t29: \"ScreenPorch\" NUMERICAL mean:0.0520865 min:-0.270208 max:8.34146 sd:1.12659\n\t30: \"TotRmsAbvGrd\" NUMERICAL mean:0.501912 min:-1.54958 max:4.60489 sd:0.974217\n\t31: \"TotalBsmtSF\" NUMERICAL mean:0.488091 min:-2.41117 max:11.5209 sd:1.07925\n\t32: \"WoodDeckSF\" NUMERICAL mean:0.269841 min:-0.752176 max:6.08764 sd:1.06645\n\t33: \"YearBuilt\" NUMERICAL mean:0.638945 min:-3.02286 max:1.28284 sd:0.73554\n\t34: \"YearRemodAdd\" NUMERICAL mean:0.556626 min:-1.68937 max:1.21784 sd:0.646849\n\t35: \"YrSold\" NUMERICAL mean:-0.0325146 min:-1.36765 max:1.64521 sd:1.00141\n\t36: \"__LABEL\" NUMERICAL mean:12.3208 min:11.3206 max:13.5345 sd:0.294801\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO kernel.cc:762] Configure learner\n[WARNING gradient_boosted_trees.cc:1643] Subsample hyperparameter given but sampling method does not match.\n[WARNING gradient_boosted_trees.cc:1656] GOSS alpha hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1665] GOSS beta hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1677] SelGB ratio hyperparameter given but SelGB is disabled.\n[INFO kernel.cc:787] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"1stFlrSF\"\nfeatures: \"2ndFlrSF\"\nfeatures: \"3SsnPorch\"\nfeatures: \"BedroomAbvGr\"\nfeatures: \"BsmtFinSF1\"\nfeatures: \"BsmtFinSF2\"\nfeatures: \"BsmtFullBath\"\nfeatures: \"BsmtHalfBath\"\nfeatures: \"BsmtUnfSF\"\nfeatures: \"EnclosedPorch\"\nfeatures: \"Fireplaces\"\nfeatures: \"FullBath\"\nfeatures: \"GarageArea\"\nfeatures: \"GarageCars\"\nfeatures: \"GarageYrBlt\"\nfeatures: \"GrLivArea\"\nfeatures: \"HalfBath\"\nfeatures: \"KitchenAbvGr\"\nfeatures: \"LotArea\"\nfeatures: \"LotFrontage\"\nfeatures: \"LowQualFinSF\"\nfeatures: \"MSSubClass\"\nfeatures: \"MasVnrArea\"\nfeatures: \"MiscVal\"\nfeatures: \"MoSold\"\nfeatures: \"OpenPorchSF\"\nfeatures: \"OverallCond\"\nfeatures: \"OverallQual\"\nfeatures: \"PoolArea\"\nfeatures: \"ScreenPorch\"\nfeatures: \"TotRmsAbvGrd\"\nfeatures: \"TotalBsmtSF\"\nfeatures: \"WoodDeckSF\"\nfeatures: \"YearBuilt\"\nfeatures: \"YearRemodAdd\"\nfeatures: \"YrSold\"\nlabel: \"__LABEL\"\ntask: REGRESSION\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 6\n    min_examples: 5\n    in_split_min_examples_check: true\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    num_candidate_attributes_ratio: -1\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n  }\n  shrinkage: 0.1\n  validation_set_ratio: 0.1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  apply_link_function: true\n  compute_permutation_variable_importance: false\n}\n\n[INFO kernel.cc:790] Deployment config:\nnum_threads: 6\n\n[INFO kernel.cc:817] Train model\n[INFO gradient_boosted_trees.cc:404] Default loss set to SQUARED_ERROR\n[INFO gradient_boosted_trees.cc:1001] Training gradient boosted tree on 642 example(s) and 36 feature(s).\n[INFO gradient_boosted_trees.cc:1044] 583 examples used for training and 59 examples used for validation\n[INFO gradient_boosted_trees.cc:1426] \tnum-trees:1 train-loss:0.272317 train-rmse:0.272317 valid-loss:0.271869 valid-rmse:0.271869\n[INFO gradient_boosted_trees.cc:1428] \tnum-trees:2 train-loss:0.251196 train-rmse:0.251196 valid-loss:0.253464 valid-rmse:0.253464\n[INFO gradient_boosted_trees.cc:2740] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 0.142356\n[INFO gradient_boosted_trees.cc:229] Truncates the model to 56 tree(s) i.e. 56  iteration(s).\n[INFO gradient_boosted_trees.cc:263] Final model num-trees:56 valid-loss:0.142356 valid-rmse:0.142356\n[INFO kernel.cc:828] Export model in log directory: /tmp/tmpmhrkeer7\n[INFO kernel.cc:836] Save model in resources\n[INFO kernel.cc:988] Loading model from path\n[INFO abstract_model.cc:993] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n[INFO kernel.cc:848] Use fast generic engine\n 70%|███████   | 7/10 [00:04<00:03,  1.12s/it][INFO kernel.cc:736] Start Yggdrasil model training\n[INFO kernel.cc:737] Collect training examples\n[INFO kernel.cc:392] Number of batches: 11\n[INFO kernel.cc:393] Number of examples: 642\n[INFO kernel.cc:759] Dataset:\nNumber of records: 642\nNumber of columns: 37\n\nNumber of columns by type:\n\tNUMERICAL: 37 (100%)\n\nColumns:\n\nNUMERICAL: 37 (100%)\n\t0: \"1stFlrSF\" NUMERICAL mean:0.462022 min:-1.72757 max:9.13268 sd:1.1049\n\t1: \"2ndFlrSF\" NUMERICAL mean:0.362677 min:-0.795163 max:3.93696 sd:1.14645\n\t2: \"3SsnPorch\" NUMERICAL mean:0.016744 min:-0.116339 max:13.771 sd:1.05451\n\t3: \"BedroomAbvGr\" NUMERICAL mean:0.22872 min:-3.51495 max:6.295 sd:1.01077\n\t4: \"BsmtFinSF1\" NUMERICAL mean:0.188362 min:-0.973018 max:11.4058 sd:1.20321\n\t5: \"BsmtFinSF2\" NUMERICAL mean:-0.097484 min:-0.288653 max:8.85164 sd:0.916389\n\t6: \"BsmtFullBath\" NUMERICAL mean:0.0928759 min:-0.819964 max:3.03558 sd:1.0095\n\t7: \"BsmtHalfBath\" NUMERICAL mean:-0.051799 min:-0.241061 max:8.13868 sd:0.901028\n\t8: \"BsmtUnfSF\" NUMERICAL mean:0.296503 min:-1.28418 max:3.51756 sd:1.10107\n\t9: \"EnclosedPorch\" NUMERICAL mean:-0.175488 min:-0.359325 max:8.67531 sd:0.834821\n\t10: \"Fireplaces\" NUMERICAL mean:0.363628 min:-0.951226 max:3.70394 sd:0.954741\n\t11: \"FullBath\" NUMERICAL mean:0.736002 min:-2.84182 max:2.60552 sd:0.650667\n\t12: \"GarageArea\" NUMERICAL mean:0.546838 min:-2.21296 max:4.42153 sd:0.795215\n\t13: \"GarageCars\" NUMERICAL mean:0.618222 min:-2.36544 max:2.98889 sd:0.654194\n\t14: \"GarageYrBlt\" NUMERICAL mean:0.259648 min:-4.12033 max:0.311426 sd:0.248065\n\t15: \"GrLivArea\" NUMERICAL mean:0.638656 min:-0.796618 max:7.85557 sd:0.949154\n\t16: \"HalfBath\" NUMERICAL mean:0.304247 min:-0.761621 max:3.21679 sd:1.03462\n\t17: \"KitchenAbvGr\" NUMERICAL mean:-0.0275893 min:-0.211454 max:4.32858 sd:0.894955\n\t18: \"LotArea\" NUMERICAL mean:0.183605 min:-0.841848 max:20.5183 sd:1.39988\n\t19: \"LotFrontage\" NUMERICAL mean:0.112036 min:-1.67235 max:7.31235 sd:1.13339\n\t20: \"LowQualFinSF\" NUMERICAL mean:-0.0273409 min:-0.120242 max:11.6477 sd:0.91874\n\t21: \"MSSubClass\" NUMERICAL mean:0.0393021 min:-0.872563 max:3.14767 sd:0.904219\n\t22: \"MasVnrArea\" NUMERICAL mean:0.337923 min:-0.576245 max:8.28509 sd:1.21611\n\t23: \"MiscVal\" NUMERICAL mean:-0.0271354 min:-0.0876878 max:16.6478 sd:0.723267\n\t24: \"MoSold\" NUMERICAL mean:0.0370742 min:-1.96911 max:2.10089 sd:1.03737\n\t25: \"OpenPorchSF\" NUMERICAL mean:0.343121 min:-0.704483 max:7.5542 sd:1.05834\n\t26: \"OverallCond\" NUMERICAL mean:-0.26516 min:-3.21403 max:3.07857 sd:0.76549\n\t27: \"OverallQual\" NUMERICAL mean:0.674012 min:-1.51847 max:2.82143 sd:0.798998\n\t28: \"PoolArea\" NUMERICAL mean:0.0199257 min:-0.0686917 max:18.3062 sd:1.13626\n\t29: \"ScreenPorch\" NUMERICAL mean:0.0605261 min:-0.270208 max:8.34146 sd:1.11909\n\t30: \"TotRmsAbvGrd\" NUMERICAL mean:0.513416 min:-1.54958 max:4.60489 sd:0.958273\n\t31: \"TotalBsmtSF\" NUMERICAL mean:0.458623 min:-2.41117 max:11.5209 sd:1.05213\n\t32: \"WoodDeckSF\" NUMERICAL mean:0.262967 min:-0.752176 max:6.08764 sd:1.07135\n\t33: \"YearBuilt\" NUMERICAL mean:0.629865 min:-3.02286 max:1.28284 sd:0.758075\n\t34: \"YearRemodAdd\" NUMERICAL mean:0.551871 min:-1.68937 max:1.21784 sd:0.660907\n\t35: \"YrSold\" NUMERICAL mean:-0.044247 min:-1.36765 max:1.64521 sd:1.00198\n\t36: \"__LABEL\" NUMERICAL mean:12.3156 min:11.3206 max:13.5345 sd:0.293335\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO kernel.cc:762] Configure learner\n[WARNING gradient_boosted_trees.cc:1643] Subsample hyperparameter given but sampling method does not match.\n[WARNING gradient_boosted_trees.cc:1656] GOSS alpha hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1665] GOSS beta hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1677] SelGB ratio hyperparameter given but SelGB is disabled.\n[INFO kernel.cc:787] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"1stFlrSF\"\nfeatures: \"2ndFlrSF\"\nfeatures: \"3SsnPorch\"\nfeatures: \"BedroomAbvGr\"\nfeatures: \"BsmtFinSF1\"\nfeatures: \"BsmtFinSF2\"\nfeatures: \"BsmtFullBath\"\nfeatures: \"BsmtHalfBath\"\nfeatures: \"BsmtUnfSF\"\nfeatures: \"EnclosedPorch\"\nfeatures: \"Fireplaces\"\nfeatures: \"FullBath\"\nfeatures: \"GarageArea\"\nfeatures: \"GarageCars\"\nfeatures: \"GarageYrBlt\"\nfeatures: \"GrLivArea\"\nfeatures: \"HalfBath\"\nfeatures: \"KitchenAbvGr\"\nfeatures: \"LotArea\"\nfeatures: \"LotFrontage\"\nfeatures: \"LowQualFinSF\"\nfeatures: \"MSSubClass\"\nfeatures: \"MasVnrArea\"\nfeatures: \"MiscVal\"\nfeatures: \"MoSold\"\nfeatures: \"OpenPorchSF\"\nfeatures: \"OverallCond\"\nfeatures: \"OverallQual\"\nfeatures: \"PoolArea\"\nfeatures: \"ScreenPorch\"\nfeatures: \"TotRmsAbvGrd\"\nfeatures: \"TotalBsmtSF\"\nfeatures: \"WoodDeckSF\"\nfeatures: \"YearBuilt\"\nfeatures: \"YearRemodAdd\"\nfeatures: \"YrSold\"\nlabel: \"__LABEL\"\ntask: REGRESSION\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 6\n    min_examples: 5\n    in_split_min_examples_check: true\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    num_candidate_attributes_ratio: -1\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n  }\n  shrinkage: 0.1\n  validation_set_ratio: 0.1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  apply_link_function: true\n  compute_permutation_variable_importance: false\n}\n\n[INFO kernel.cc:790] Deployment config:\nnum_threads: 6\n\n[INFO kernel.cc:817] Train model\n[INFO gradient_boosted_trees.cc:404] Default loss set to SQUARED_ERROR\n[INFO gradient_boosted_trees.cc:1001] Training gradient boosted tree on 642 example(s) and 36 feature(s).\n[INFO gradient_boosted_trees.cc:1044] 583 examples used for training and 59 examples used for validation\n[INFO gradient_boosted_trees.cc:1426] \tnum-trees:1 train-loss:0.271099 train-rmse:0.271099 valid-loss:0.265056 valid-rmse:0.265056\n[INFO gradient_boosted_trees.cc:2740] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 0.201696\n[INFO gradient_boosted_trees.cc:229] Truncates the model to 20 tree(s) i.e. 20  iteration(s).\n[INFO gradient_boosted_trees.cc:263] Final model num-trees:20 valid-loss:0.201696 valid-rmse:0.201696\n[INFO kernel.cc:828] Export model in log directory: /tmp/tmp1t9cbzld\n[INFO kernel.cc:836] Save model in resources\n[INFO kernel.cc:988] Loading model from path\n[INFO kernel.cc:848] Use fast generic engine\n100%|██████████| 10/10 [00:11<00:00,  1.13s/it]\n 60%|██████    | 6/10 [00:01<00:01,  2.96it/s]/opt/conda/lib/python3.7/site-packages/tensorflow_decision_forests/keras/core.py:1612: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only\n  features_dataframe = dataframe.drop(label, 1)\n[INFO kernel.cc:736] Start Yggdrasil model training\n[INFO kernel.cc:737] Collect training examples\n[INFO kernel.cc:392] Number of batches: 11\n[INFO kernel.cc:393] Number of examples: 671\n[INFO kernel.cc:759] Dataset:\nNumber of records: 671\nNumber of columns: 37\n\nNumber of columns by type:\n\tNUMERICAL: 37 (100%)\n\nColumns:\n\nNUMERICAL: 37 (100%)\n\t0: \"1stFlrSF\" NUMERICAL mean:-0.444209 min:-2.14417 max:1.67774 sd:0.612734\n\t1: \"2ndFlrSF\" NUMERICAL mean:-0.321997 min:-0.795163 max:1.98911 sd:0.701291\n\t2: \"3SsnPorch\" NUMERICAL mean:-0.0366045 min:-0.116339 max:10.8024 sd:0.724943\n\t3: \"BedroomAbvGr\" NUMERICAL mean:-0.187098 min:-3.51495 max:3.84251 sd:0.957436\n\t4: \"BsmtFinSF1\" NUMERICAL mean:-0.185415 min:-0.973018 max:2.18528 sd:0.716567\n\t5: \"BsmtFinSF2\" NUMERICAL mean:0.0971591 min:-0.288653 max:6.65648 sd:1.07877\n\t6: \"BsmtFullBath\" NUMERICAL mean:-0.0758606 min:-0.819964 max:4.96336 sd:0.984576\n\t7: \"BsmtHalfBath\" NUMERICAL mean:0.0586614 min:-0.241061 max:8.13868 sd:1.10376\n\t8: \"BsmtUnfSF\" NUMERICAL mean:-0.282884 min:-1.28418 max:2.79084 sd:0.750743\n\t9: \"EnclosedPorch\" NUMERICAL mean:0.19901 min:-0.359325 max:4.4526 sd:1.14294\n\t10: \"Fireplaces\" NUMERICAL mean:-0.368464 min:-0.951226 max:2.15222 sd:0.891254\n\t11: \"FullBath\" NUMERICAL mean:-0.709429 min:-2.84182 max:2.60552 sd:0.743823\n\t12: \"GarageArea\" NUMERICAL mean:-0.532205 min:-2.21296 max:3.62614 sd:0.870134\n\t13: \"GarageCars\" NUMERICAL mean:-0.595958 min:-2.36544 max:2.98889 sd:0.907588\n\t14: \"GarageYrBlt\" NUMERICAL mean:-0.249482 min:-4.12033 max:0.307018 sd:1.33232\n\t15: \"GrLivArea\" NUMERICAL mean:-0.589657 min:-2.24912 max:1.92373 sd:0.616708\n\t16: \"HalfBath\" NUMERICAL mean:-0.287294 min:-0.761621 max:3.21679 sd:0.875057\n\t17: \"KitchenAbvGr\" NUMERICAL mean:0.0118267 min:-4.75149 max:8.86861 sd:1.07152\n\t18: \"LotArea\" NUMERICAL mean:-0.161101 min:-0.90599 max:4.67868 sd:0.425109\n\t19: \"LotFrontage\" NUMERICAL mean:-0.105636 min:-1.67235 max:2.73416 sd:0.863159\n\t20: \"LowQualFinSF\" NUMERICAL mean:0.0500485 min:-0.120242 max:10.7425 sd:1.16551\n\t21: \"MSSubClass\" NUMERICAL mean:-0.0533257 min:-0.872563 max:3.14767 sd:1.06629\n\t22: \"MasVnrArea\" NUMERICAL mean:-0.318531 min:-0.57071 max:5.2243 sd:0.583932\n\t23: \"MiscVal\" NUMERICAL mean:0.0343551 min:-0.0876878 max:31.1653 sd:1.28228\n\t24: \"MoSold\" NUMERICAL mean:-0.0617624 min:-1.96911 max:2.10089 sd:0.964957\n\t25: \"OpenPorchSF\" NUMERICAL mean:-0.357474 min:-0.704483 max:7.19184 sd:0.768539\n\t26: \"OverallCond\" NUMERICAL mean:0.271887 min:-4.11297 max:3.07857 sd:1.14151\n\t27: \"OverallQual\" NUMERICAL mean:-0.657172 min:-3.68841 max:1.37479 sd:0.668751\n\t28: \"PoolArea\" NUMERICAL mean:-0.0232739 min:-0.0686917 max:16.0653 sd:0.832103\n\t29: \"ScreenPorch\" NUMERICAL mean:-0.0389007 min:-0.270208 max:6.63707 sd:0.88729\n\t30: \"TotRmsAbvGrd\" NUMERICAL mean:-0.470023 min:-2.78047 max:2.75855 sd:0.772964\n\t31: \"TotalBsmtSF\" NUMERICAL mean:-0.441961 min:-2.41117 max:1.69322 sd:0.670975\n\t32: \"WoodDeckSF\" NUMERICAL mean:-0.239815 min:-0.752176 max:5.12192 sd:0.864673\n\t33: \"YearBuilt\" NUMERICAL mean:-0.62765 min:-3.28782 max:1.18348 sd:0.826559\n\t34: \"YearRemodAdd\" NUMERICAL mean:-0.52374 min:-1.68937 max:1.21784 sd:0.985155\n\t35: \"YrSold\" NUMERICAL mean:0.0489753 min:-1.36765 max:1.64521 sd:0.995652\n\t36: \"__LABEL\" NUMERICAL mean:11.7422 min:10.4602 max:12.4684 sd:0.264743\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO kernel.cc:762] Configure learner\n[WARNING gradient_boosted_trees.cc:1643] Subsample hyperparameter given but sampling method does not match.\n[WARNING gradient_boosted_trees.cc:1656] GOSS alpha hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1665] GOSS beta hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1677] SelGB ratio hyperparameter given but SelGB is disabled.\n[INFO kernel.cc:787] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"1stFlrSF\"\nfeatures: \"2ndFlrSF\"\nfeatures: \"3SsnPorch\"\nfeatures: \"BedroomAbvGr\"\nfeatures: \"BsmtFinSF1\"\nfeatures: \"BsmtFinSF2\"\nfeatures: \"BsmtFullBath\"\nfeatures: \"BsmtHalfBath\"\nfeatures: \"BsmtUnfSF\"\nfeatures: \"EnclosedPorch\"\nfeatures: \"Fireplaces\"\nfeatures: \"FullBath\"\nfeatures: \"GarageArea\"\nfeatures: \"GarageCars\"\nfeatures: \"GarageYrBlt\"\nfeatures: \"GrLivArea\"\nfeatures: \"HalfBath\"\nfeatures: \"KitchenAbvGr\"\nfeatures: \"LotArea\"\nfeatures: \"LotFrontage\"\nfeatures: \"LowQualFinSF\"\nfeatures: \"MSSubClass\"\nfeatures: \"MasVnrArea\"\nfeatures: \"MiscVal\"\nfeatures: \"MoSold\"\nfeatures: \"OpenPorchSF\"\nfeatures: \"OverallCond\"\nfeatures: \"OverallQual\"\nfeatures: \"PoolArea\"\nfeatures: \"ScreenPorch\"\nfeatures: \"TotRmsAbvGrd\"\nfeatures: \"TotalBsmtSF\"\nfeatures: \"WoodDeckSF\"\nfeatures: \"YearBuilt\"\nfeatures: \"YearRemodAdd\"\nfeatures: \"YrSold\"\nlabel: \"__LABEL\"\ntask: REGRESSION\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 6\n    min_examples: 5\n    in_split_min_examples_check: true\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    num_candidate_attributes_ratio: -1\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n  }\n  shrinkage: 0.1\n  validation_set_ratio: 0.1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  apply_link_function: true\n  compute_permutation_variable_importance: false\n}\n\n[INFO kernel.cc:790] Deployment config:\nnum_threads: 6\n\n[INFO kernel.cc:817] Train model\n[INFO gradient_boosted_trees.cc:404] Default loss set to SQUARED_ERROR\n[INFO gradient_boosted_trees.cc:1001] Training gradient boosted tree on 671 example(s) and 36 feature(s).\n[INFO gradient_boosted_trees.cc:1044] 609 examples used for training and 62 examples used for validation\n[INFO gradient_boosted_trees.cc:1426] \tnum-trees:1 train-loss:0.250015 train-rmse:0.250015 valid-loss:0.238094 valid-rmse:0.238094\n[INFO gradient_boosted_trees.cc:2740] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 0.116294\n[INFO gradient_boosted_trees.cc:229] Truncates the model to 84 tree(s) i.e. 84  iteration(s).\n[INFO gradient_boosted_trees.cc:263] Final model num-trees:84 valid-loss:0.116294 valid-rmse:0.116294\n[INFO kernel.cc:828] Export model in log directory: /tmp/tmpz74kvpds\n[INFO kernel.cc:836] Save model in resources\n[INFO kernel.cc:988] Loading model from path\n[INFO abstract_model.cc:993] Engine \"GradientBoostedTreesQuickScorerExtended\" built\n[INFO kernel.cc:848] Use fast generic engine\n 70%|███████   | 7/10 [00:04<00:03,  1.15s/it][INFO kernel.cc:736] Start Yggdrasil model training\n[INFO kernel.cc:737] Collect training examples\n[INFO kernel.cc:392] Number of batches: 11\n[INFO kernel.cc:393] Number of examples: 671\n[INFO kernel.cc:759] Dataset:\nNumber of records: 671\nNumber of columns: 37\n\nNumber of columns by type:\n\tNUMERICAL: 37 (100%)\n\nColumns:\n\nNUMERICAL: 37 (100%)\n\t0: \"1stFlrSF\" NUMERICAL mean:-0.448131 min:-2.04584 max:1.67774 sd:0.612752\n\t1: \"2ndFlrSF\" NUMERICAL mean:-0.348297 min:-0.795163 max:1.98911 sd:0.686066\n\t2: \"3SsnPorch\" NUMERICAL mean:-0.0457577 min:-0.116339 max:10.8024 sd:0.686077\n\t3: \"BedroomAbvGr\" NUMERICAL mean:-0.23644 min:-3.51495 max:3.84251 sd:0.941459\n\t4: \"BsmtFinSF1\" NUMERICAL mean:-0.168026 min:-0.973018 max:2.18528 sd:0.716064\n\t5: \"BsmtFinSF2\" NUMERICAL mean:0.0818738 min:-0.288653 max:6.65648 sd:1.05867\n\t6: \"BsmtFullBath\" NUMERICAL mean:-0.0729876 min:-0.819964 max:4.96336 sd:0.990818\n\t7: \"BsmtHalfBath\" NUMERICAL mean:0.0649057 min:-0.241061 max:3.94881 sd:1.09011\n\t8: \"BsmtUnfSF\" NUMERICAL mean:-0.303634 min:-1.28418 max:2.79084 sd:0.742432\n\t9: \"EnclosedPorch\" NUMERICAL mean:0.194083 min:-0.359325 max:4.4526 sd:1.13848\n\t10: \"Fireplaces\" NUMERICAL mean:-0.361526 min:-0.951226 max:2.15222 sd:0.900732\n\t11: \"FullBath\" NUMERICAL mean:-0.709429 min:-2.84182 max:2.60552 sd:0.7504\n\t12: \"GarageArea\" NUMERICAL mean:-0.524556 min:-2.21296 max:3.62614 sd:0.875645\n\t13: \"GarageCars\" NUMERICAL mean:-0.585983 min:-2.36544 max:2.98889 sd:0.907211\n\t14: \"GarageYrBlt\" NUMERICAL mean:-0.242581 min:-4.12033 max:0.307018 sd:1.32419\n\t15: \"GrLivArea\" NUMERICAL mean:-0.617213 min:-2.05114 max:1.92373 sd:0.5999\n\t16: \"HalfBath\" NUMERICAL mean:-0.302117 min:-0.761621 max:3.21679 sd:0.866072\n\t17: \"KitchenAbvGr\" NUMERICAL mean:0.0321249 min:-0.211454 max:8.86861 sd:1.0526\n\t18: \"LotArea\" NUMERICAL mean:-0.15863 min:-0.923729 max:4.67868 sd:0.418136\n\t19: \"LotFrontage\" NUMERICAL mean:-0.116041 min:-1.67235 max:2.73416 sd:0.854454\n\t20: \"LowQualFinSF\" NUMERICAL mean:0.019541 min:-0.120242 max:10.7425 sd:1.02727\n\t21: \"MSSubClass\" NUMERICAL mean:-0.0568501 min:-0.872563 max:3.14767 sd:1.06089\n\t22: \"MasVnrArea\" NUMERICAL mean:-0.336274 min:-0.57071 max:5.2243 sd:0.559416\n\t23: \"MiscVal\" NUMERICAL mean:0.0345054 min:-0.0876878 max:31.1653 sd:1.28228\n\t24: \"MoSold\" NUMERICAL mean:-0.0584539 min:-1.96911 max:2.10089 sd:0.962214\n\t25: \"OpenPorchSF\" NUMERICAL mean:-0.345593 min:-0.704483 max:7.19184 sd:0.784886\n\t26: \"OverallCond\" NUMERICAL mean:0.273227 min:-4.11297 max:3.07857 sd:1.13264\n\t27: \"OverallQual\" NUMERICAL mean:-0.656094 min:-3.68841 max:1.37479 sd:0.657354\n\t28: \"PoolArea\" NUMERICAL mean:-0.0473186 min:-0.0686917 max:14.2727 sd:0.553229\n\t29: \"ScreenPorch\" NUMERICAL mean:-0.0467884 min:-0.270208 max:6.63707 sd:0.867934\n\t30: \"TotRmsAbvGrd\" NUMERICAL mean:-0.492036 min:-2.16502 max:2.1431 sd:0.749734\n\t31: \"TotalBsmtSF\" NUMERICAL mean:-0.450403 min:-2.41117 max:1.69322 sd:0.682422\n\t32: \"WoodDeckSF\" NUMERICAL mean:-0.257859 min:-0.752176 max:5.12192 sd:0.865053\n\t33: \"YearBuilt\" NUMERICAL mean:-0.604994 min:-3.28782 max:1.18348 sd:0.816517\n\t34: \"YearRemodAdd\" NUMERICAL mean:-0.515075 min:-1.68937 max:1.21784 sd:0.991367\n\t35: \"YrSold\" NUMERICAL mean:0.0242797 min:-1.36765 max:1.64521 sd:0.990558\n\t36: \"__LABEL\" NUMERICAL mean:11.7453 min:10.4602 max:12.4684 sd:0.261282\n\nTerminology:\n\tnas: Number of non-available (i.e. missing) values.\n\tood: Out of dictionary.\n\tmanually-defined: Attribute which type is manually defined by the user i.e. the type was not automatically inferred.\n\ttokenized: The attribute value is obtained through tokenization.\n\thas-dict: The attribute is attached to a string dictionary e.g. a categorical attribute stored as a string.\n\tvocab-size: Number of unique values.\n\n[INFO kernel.cc:762] Configure learner\n[WARNING gradient_boosted_trees.cc:1643] Subsample hyperparameter given but sampling method does not match.\n[WARNING gradient_boosted_trees.cc:1656] GOSS alpha hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1665] GOSS beta hyperparameter given but GOSS is disabled.\n[WARNING gradient_boosted_trees.cc:1677] SelGB ratio hyperparameter given but SelGB is disabled.\n[INFO kernel.cc:787] Training config:\nlearner: \"GRADIENT_BOOSTED_TREES\"\nfeatures: \"1stFlrSF\"\nfeatures: \"2ndFlrSF\"\nfeatures: \"3SsnPorch\"\nfeatures: \"BedroomAbvGr\"\nfeatures: \"BsmtFinSF1\"\nfeatures: \"BsmtFinSF2\"\nfeatures: \"BsmtFullBath\"\nfeatures: \"BsmtHalfBath\"\nfeatures: \"BsmtUnfSF\"\nfeatures: \"EnclosedPorch\"\nfeatures: \"Fireplaces\"\nfeatures: \"FullBath\"\nfeatures: \"GarageArea\"\nfeatures: \"GarageCars\"\nfeatures: \"GarageYrBlt\"\nfeatures: \"GrLivArea\"\nfeatures: \"HalfBath\"\nfeatures: \"KitchenAbvGr\"\nfeatures: \"LotArea\"\nfeatures: \"LotFrontage\"\nfeatures: \"LowQualFinSF\"\nfeatures: \"MSSubClass\"\nfeatures: \"MasVnrArea\"\nfeatures: \"MiscVal\"\nfeatures: \"MoSold\"\nfeatures: \"OpenPorchSF\"\nfeatures: \"OverallCond\"\nfeatures: \"OverallQual\"\nfeatures: \"PoolArea\"\nfeatures: \"ScreenPorch\"\nfeatures: \"TotRmsAbvGrd\"\nfeatures: \"TotalBsmtSF\"\nfeatures: \"WoodDeckSF\"\nfeatures: \"YearBuilt\"\nfeatures: \"YearRemodAdd\"\nfeatures: \"YrSold\"\nlabel: \"__LABEL\"\ntask: REGRESSION\n[yggdrasil_decision_forests.model.gradient_boosted_trees.proto.gradient_boosted_trees_config] {\n  num_trees: 300\n  decision_tree {\n    max_depth: 6\n    min_examples: 5\n    in_split_min_examples_check: true\n    missing_value_policy: GLOBAL_IMPUTATION\n    allow_na_conditions: false\n    categorical_set_greedy_forward {\n      sampling: 0.1\n      max_num_items: -1\n      min_item_frequency: 1\n    }\n    growing_strategy_local {\n    }\n    categorical {\n      cart {\n      }\n    }\n    num_candidate_attributes_ratio: -1\n    axis_aligned_split {\n    }\n    internal {\n      sorting_strategy: PRESORTED\n    }\n  }\n  shrinkage: 0.1\n  validation_set_ratio: 0.1\n  early_stopping: VALIDATION_LOSS_INCREASE\n  early_stopping_num_trees_look_ahead: 30\n  l2_regularization: 0\n  lambda_loss: 1\n  mart {\n  }\n  adapt_subsample_for_maximum_training_duration: false\n  l1_regularization: 0\n  use_hessian_gain: false\n  l2_regularization_categorical: 1\n  apply_link_function: true\n  compute_permutation_variable_importance: false\n}\n\n[INFO kernel.cc:790] Deployment config:\nnum_threads: 6\n\n[INFO kernel.cc:817] Train model\n[INFO gradient_boosted_trees.cc:404] Default loss set to SQUARED_ERROR\n[INFO gradient_boosted_trees.cc:1001] Training gradient boosted tree on 671 example(s) and 36 feature(s).\n[INFO gradient_boosted_trees.cc:1044] 609 examples used for training and 62 examples used for validation\n[INFO gradient_boosted_trees.cc:1426] \tnum-trees:1 train-loss:0.249026 train-rmse:0.249026 valid-loss:0.200850 valid-rmse:0.200850\n[INFO gradient_boosted_trees.cc:2740] Early stop of the training because the validation loss does not decrease anymore. Best valid-loss: 0.104134\n[INFO gradient_boosted_trees.cc:229] Truncates the model to 81 tree(s) i.e. 81  iteration(s).\n[INFO gradient_boosted_trees.cc:263] Final model num-trees:81 valid-loss:0.104134 valid-rmse:0.104134\n[INFO kernel.cc:828] Export model in log directory: /tmp/tmpcp5qii17\n[INFO kernel.cc:836] Save model in resources\n[INFO kernel.cc:988] Loading model from path\n[INFO kernel.cc:848] Use fast generic engine\n100%|██████████| 10/10 [00:11<00:00,  1.19s/it]\n","output_type":"stream"}]},{"cell_type":"code","source":"experts_rmses_tr","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:14:50.376810Z","iopub.execute_input":"2022-08-31T17:14:50.377643Z","iopub.status.idle":"2022-08-31T17:14:50.385684Z","shell.execute_reply.started":"2022-08-31T17:14:50.377597Z","shell.execute_reply":"2022-08-31T17:14:50.384070Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"[1.0455093569627287, 1.0427789119480244]"},"metadata":{}}]},{"cell_type":"code","source":"experts_rmses_val","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:15:07.352688Z","iopub.execute_input":"2022-08-31T17:15:07.353160Z","iopub.status.idle":"2022-08-31T17:15:07.360796Z","shell.execute_reply.started":"2022-08-31T17:15:07.353117Z","shell.execute_reply":"2022-08-31T17:15:07.359589Z"},"trusted":true},"execution_count":96,"outputs":[{"execution_count":96,"output_type":"execute_result","data":{"text/plain":"[1.1496983379286516, 1.1614916488904745]"},"metadata":{}}]},{"cell_type":"code","source":"avg_experts_preds_test = np.mean(experts_preds_test, axis=0)\navg_experts_preds_test","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:16:22.736722Z","iopub.execute_input":"2022-08-31T17:16:22.737623Z","iopub.status.idle":"2022-08-31T17:16:22.747067Z","shell.execute_reply.started":"2022-08-31T17:16:22.737574Z","shell.execute_reply":"2022-08-31T17:16:22.745484Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"array([142523.91729965, 163415.35183128, 179599.84978026, ...,\n       188298.11941758, 136007.41864971, 205206.74046762])"},"metadata":{}}]},{"cell_type":"code","source":"submit_weighted_ensemble = pd.DataFrame()\nsubmit_weighted_ensemble['Id'] = test_data['Id']\nsubmit_weighted_ensemble['SalePrice'] = avg_experts_preds_test\n\nsubmit_weighted_ensemble.to_csv('/kaggle/working/mixture_of_experts_weighted_ensemble.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-08-31T17:17:36.943764Z","iopub.execute_input":"2022-08-31T17:17:36.944395Z","iopub.status.idle":"2022-08-31T17:17:36.961665Z","shell.execute_reply.started":"2022-08-31T17:17:36.944342Z","shell.execute_reply":"2022-08-31T17:17:36.960461Z"},"trusted":true},"execution_count":98,"outputs":[]}]}