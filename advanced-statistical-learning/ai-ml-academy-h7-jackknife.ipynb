{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-06T12:36:40.191286Z","iopub.execute_input":"2022-09-06T12:36:40.191833Z","iopub.status.idle":"2022-09-06T12:36:40.203770Z","shell.execute_reply.started":"2022-09-06T12:36:40.191776Z","shell.execute_reply":"2022-09-06T12:36:40.202401Z"},"trusted":true},"execution_count":63,"outputs":[]},{"cell_type":"markdown","source":"# Homework description\n\nTake data from kaggle housing.\n1. Confident ellipsoids by sampling\n    - Create each model from the list [linear,ridge] H=1000 times on subsamples (test_size=20%)\n    - Collect train and test metrics\n    - Build 2D confident sets for metrics and visualize them (2 model -> two 2D\nsets)\n2. Do the same using jackknife\n3. Fit stacked generalization for both models and try in kaggle","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:36:41.924297Z","iopub.execute_input":"2022-09-06T12:36:41.924741Z","iopub.status.idle":"2022-09-06T12:36:41.998100Z","shell.execute_reply.started":"2022-09-06T12:36:41.924705Z","shell.execute_reply":"2022-09-06T12:36:41.996817Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"y_train = np.log(train_data['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:36:43.303162Z","iopub.execute_input":"2022-09-06T12:36:43.303701Z","iopub.status.idle":"2022-09-06T12:36:43.311010Z","shell.execute_reply.started":"2022-09-06T12:36:43.303658Z","shell.execute_reply":"2022-09-06T12:36:43.309991Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"def rmse(a, b):\n    return ((a - b) ** 2).mean() ** 0.5","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:36:44.799953Z","iopub.execute_input":"2022-09-06T12:36:44.800438Z","iopub.status.idle":"2022-09-06T12:36:44.806331Z","shell.execute_reply.started":"2022-09-06T12:36:44.800398Z","shell.execute_reply":"2022-09-06T12:36:44.805231Z"},"trusted":true},"execution_count":66,"outputs":[]},{"cell_type":"code","source":"numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\nx_train = train_data[numeric_columns].fillna(-1)\nx_test = test_data[numeric_columns].fillna(-1)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:36:47.007432Z","iopub.execute_input":"2022-09-06T12:36:47.007862Z","iopub.status.idle":"2022-09-06T12:36:47.021690Z","shell.execute_reply.started":"2022-09-06T12:36:47.007826Z","shell.execute_reply":"2022-09-06T12:36:47.019871Z"},"trusted":true},"execution_count":67,"outputs":[]},{"cell_type":"code","source":"from sklearn.linear_model import LinearRegression, Ridge\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:36:49.201719Z","iopub.execute_input":"2022-09-06T12:36:49.202985Z","iopub.status.idle":"2022-09-06T12:36:49.208270Z","shell.execute_reply.started":"2022-09-06T12:36:49.202940Z","shell.execute_reply":"2022-09-06T12:36:49.207210Z"},"trusted":true},"execution_count":68,"outputs":[]},{"cell_type":"code","source":"H = 1000\n\ndef get_metrics(model):\n    metrics = []\n    for h in tqdm(range(H)):\n        x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train, test_size=0.2)\n        model = model.fit(x_tr, y_tr)\n        y_pred_tr = model.predict(x_tr)\n        y_pred_val = model.predict(x_val)\n\n        metrics.append({\n            'algorithm' : f'{type(model).__name__}',\n            'train_mse' : rmse(y_pred_tr, y_tr),\n            'val_mse' : rmse(y_pred_val, y_val),\n        })\n        \n    return metrics\n\nlinear_metrics = pd.DataFrame(get_metrics(LinearRegression()))\nlinear_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T11:51:07.515458Z","iopub.execute_input":"2022-09-06T11:51:07.515918Z","iopub.status.idle":"2022-09-06T11:51:40.865330Z","shell.execute_reply.started":"2022-09-06T11:51:07.515882Z","shell.execute_reply":"2022-09-06T11:51:40.863659Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"from scipy.stats import chi2\n\nalpha = 0.05\nr = chi2.ppf(1 - alpha, 2) ** 0.5","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:37:01.530942Z","iopub.execute_input":"2022-09-06T12:37:01.531577Z","iopub.status.idle":"2022-09-06T12:37:01.538462Z","shell.execute_reply.started":"2022-09-06T12:37:01.531539Z","shell.execute_reply":"2022-09-06T12:37:01.537257Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"markdown","source":"Based on scikit-learn docs:","metadata":{}},{"cell_type":"code","source":"from matplotlib.patches import Ellipse\nimport matplotlib.transforms as transforms\n\ndef confidence_ellipse(x, y, ax, n_std=3.0, facecolor='none', **kwargs):\n    \"\"\"\n    Create a plot of the covariance confidence ellipse of *x* and *y*.\n\n    Parameters\n    ----------\n    x, y : array-like, shape (n, )\n        Input data.\n\n    ax : matplotlib.axes.Axes\n        The axes object to draw the ellipse into.\n\n    n_std : float\n        The number of standard deviations to determine the ellipse's radiuses.\n\n    **kwargs\n        Forwarded to `~matplotlib.patches.Ellipse`\n\n    Returns\n    -------\n    matplotlib.patches.Ellipse\n    \"\"\"\n    if x.size != y.size:\n        raise ValueError(\"x and y must be the same size\")\n\n    cov = np.cov(x, y)\n    pearson = cov[0, 1]/np.sqrt(cov[0, 0] * cov[1, 1])\n    # Using a special case to obtain the eigenvalues of this\n    # two-dimensionl dataset.\n    ell_radius_x = np.sqrt(1 + pearson)\n    ell_radius_y = np.sqrt(1 - pearson)\n    ellipse = Ellipse((0, 0), width=ell_radius_x * 2, height=ell_radius_y * 2,\n                      facecolor=facecolor, **kwargs)\n\n    # Calculating the stdandard deviation of x from\n    # the squareroot of the variance and multiplying\n    # with the given number of standard deviations.\n    scale_x = np.sqrt(cov[0, 0]) * n_std\n    mean_x = np.mean(x)\n\n    # calculating the stdandard deviation of y ...\n    scale_y = np.sqrt(cov[1, 1]) * n_std\n    mean_y = np.mean(y)\n\n    transf = transforms.Affine2D() \\\n        .rotate_deg(45) \\\n        .scale(scale_x, scale_y) \\\n        .translate(mean_x, mean_y)\n\n    ellipse.set_transform(transf + ax.transData)\n    return ax.add_patch(ellipse)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:37:03.110135Z","iopub.execute_input":"2022-09-06T12:37:03.110560Z","iopub.status.idle":"2022-09-06T12:37:03.125020Z","shell.execute_reply.started":"2022-09-06T12:37:03.110528Z","shell.execute_reply":"2022-09-06T12:37:03.123638Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"ridge_metrics = pd.DataFrame(get_metrics(Ridge()))\nridge_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T11:53:16.303814Z","iopub.execute_input":"2022-09-06T11:53:16.304296Z","iopub.status.idle":"2022-09-06T11:53:36.691766Z","shell.execute_reply.started":"2022-09-06T11:53:16.304261Z","shell.execute_reply":"2022-09-06T11:53:36.690513Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"Intersection of Linear and Ridge Regression with alpha = 1 (default)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\nax.scatter(linear_metrics['train_mse'], linear_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(linear_metrics['train_mse'], linear_metrics['val_mse'], ax, n_std=r, edgecolor='red')\nax.scatter(ridge_metrics['train_mse'], ridge_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_metrics['train_mse'], ridge_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\nax.legend(['linear', 'linear', 'ridge alpha=1', 'ridge alpha=1'])\nax.set_title('Train and validation confident ellipses for LinReg and Ridge alpha=1')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:05:08.529069Z","iopub.execute_input":"2022-09-06T12:05:08.529548Z","iopub.status.idle":"2022-09-06T12:05:08.913896Z","shell.execute_reply.started":"2022-09-06T12:05:08.529502Z","shell.execute_reply":"2022-09-06T12:05:08.912487Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- nearly the same","metadata":{}},{"cell_type":"code","source":"ridge_100_metrics = pd.DataFrame(get_metrics(Ridge(alpha=100)))\nridge_100_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T11:59:21.435364Z","iopub.execute_input":"2022-09-06T11:59:21.435860Z","iopub.status.idle":"2022-09-06T11:59:41.759699Z","shell.execute_reply.started":"2022-09-06T11:59:21.435822Z","shell.execute_reply":"2022-09-06T11:59:41.758273Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":"Intersection of Linear and Ridge Regression with alpha = 1 (default)","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\nax.scatter(linear_metrics['train_mse'], linear_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(linear_metrics['train_mse'], linear_metrics['val_mse'], ax, n_std=r, edgecolor='red')\nax.scatter(ridge_100_metrics['train_mse'], ridge_100_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_100_metrics['train_mse'], ridge_100_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\nax.set_ylim((0.05, 0.3))\nax.legend(['linear', 'linear', 'ridge alpha=100', 'ridge alpha=100'])\nax.set_title('Train and validation confident ellipses for LinReg and Ridge alpha=100')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:04:34.229660Z","iopub.execute_input":"2022-09-06T12:04:34.230324Z","iopub.status.idle":"2022-09-06T12:04:34.628793Z","shell.execute_reply.started":"2022-09-06T12:04:34.230268Z","shell.execute_reply":"2022-09-06T12:04:34.627053Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- linear regression is more stable\n- mostly similar models' performances, but but in some small part of cases they can be different","metadata":{}},{"cell_type":"markdown","source":"### Jackknife (Leave One Out)","metadata":{}},{"cell_type":"code","source":"H = 1000\n\ndef get_jackknife_metrics(models):\n    metrics = []\n    x_train_numpy = x_train.to_numpy()\n    y_train_numpy = y_train.to_numpy()\n    for i in tqdm(range(x_train_numpy.shape[0])):\n        x_tr = np.delete(x_train_numpy, [i], axis=0)\n        y_tr = np.delete(y_train_numpy, [i])\n        x_val = x_train_numpy[[i]]\n        y_val = y_train_numpy[i]\n        for model_name, model in models.items():\n            model = model.fit(x_tr, y_tr)\n            y_pred_tr = model.predict(x_tr)\n            y_pred_val = model.predict(x_val)\n\n            metrics.append({\n                'algorithm' : model_name,\n                'train_mse' : rmse(y_pred_tr, y_tr),\n                'val_mse' : rmse(y_pred_val, y_val),\n            })\n        \n    return metrics\n\nmodels = {\n    'Linear' : LinearRegression(),\n    'Ridge alpha=1' : Ridge(),\n    'Ridge alpha=100' : Ridge(alpha=100)\n}\n\njack_metrics = get_jackknife_metrics(models)\njack_metrics = pd.DataFrame(jack_metrics)\njack_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T12:52:53.830040Z","iopub.execute_input":"2022-09-06T12:52:53.830580Z","iopub.status.idle":"2022-09-06T12:52:53.849954Z","shell.execute_reply.started":"2022-09-06T12:52:53.830539Z","shell.execute_reply":"2022-09-06T12:52:53.849058Z"},"trusted":true},"execution_count":90,"outputs":[]},{"cell_type":"code","source":"linear_jack_metrics = jack_metrics[jack_metrics['algorithm'] == 'Linear']\nridge_jack_metrics = jack_metrics[jack_metrics['algorithm'] == 'Ridge alpha=1']\nridge_100_jack_metrics = jack_metrics[jack_metrics['algorithm'] == 'Ridge alpha=100']\nlinear_jack_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:01:21.705626Z","iopub.execute_input":"2022-09-06T13:01:21.706878Z","iopub.status.idle":"2022-09-06T13:01:21.731232Z","shell.execute_reply.started":"2022-09-06T13:01:21.706830Z","shell.execute_reply":"2022-09-06T13:01:21.729976Z"},"trusted":true},"execution_count":98,"outputs":[]},{"cell_type":"code","source":"ridge_jack_metrics","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:01:24.412997Z","iopub.execute_input":"2022-09-06T13:01:24.413822Z","iopub.status.idle":"2022-09-06T13:01:24.431873Z","shell.execute_reply.started":"2022-09-06T13:01:24.413777Z","shell.execute_reply":"2022-09-06T13:01:24.430768Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n\nax.scatter(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], ax, n_std=r, edgecolor='red')\n\nax.scatter(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\n\nax.scatter(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], c='green', alpha=0.1)\nconfidence_ellipse(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], ax, n_std=r, edgecolor='green')\n\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\nax.legend(['linear', 'linear', 'ridge alpha=1', 'ridge alpha=1','ridge alpha=100', 'ridge alpha=100'])\nax.set_title('Confident ellipses using Jackknife for LinReg, Ridge alpha=1, Ridge alpha=100')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:01:30.583549Z","iopub.execute_input":"2022-09-06T13:01:30.584015Z","iopub.status.idle":"2022-09-06T13:01:31.039984Z","shell.execute_reply.started":"2022-09-06T13:01:30.583981Z","shell.execute_reply":"2022-09-06T13:01:31.038490Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"Focus on ellipsoids on the next plot","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n\nax.scatter(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], ax, n_std=r, edgecolor='red')\n\nax.scatter(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\n\nax.scatter(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], c='green', alpha=0.1)\nconfidence_ellipse(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], ax, n_std=r, edgecolor='green')\n\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\nax.set_ylim((0, 1))\nax.set_xlim((0.142, 0.148))\n\nax.legend(['linear', 'linear', 'ridge alpha=1', 'ridge alpha=1','ridge alpha=100', 'ridge alpha=100'])\nax.set_title('Confident ellipses using Jackknife for LinReg, Ridge alpha=1, Ridge alpha=100')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:03:48.493666Z","iopub.execute_input":"2022-09-06T13:03:48.494545Z","iopub.status.idle":"2022-09-06T13:03:48.953242Z","shell.execute_reply.started":"2022-09-06T13:03:48.494497Z","shell.execute_reply":"2022-09-06T13:03:48.951835Z"},"trusted":true},"execution_count":101,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- linear regression and ridge regression with alpha=1 almost identical\n- ridge with alpha=100 is different\n- test performances are the same","metadata":{}},{"cell_type":"markdown","source":"### Boostrap vs Jackknife","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n\nax.scatter(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(linear_jack_metrics['train_mse'], linear_jack_metrics['val_mse'], ax, n_std=r, edgecolor='red')\n\nax.scatter(linear_metrics['train_mse'], linear_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(linear_metrics['train_mse'], linear_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\n\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\n\nax.legend(['linear jackknife', 'linear jackknife', 'linear bootstrap', 'linear bootstrap'])\nax.set_title('Bootstrap vs Jackknife for Linear Regression')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:08:39.509441Z","iopub.execute_input":"2022-09-06T13:08:39.510721Z","iopub.status.idle":"2022-09-06T13:08:39.886801Z","shell.execute_reply.started":"2022-09-06T13:08:39.510665Z","shell.execute_reply":"2022-09-06T13:08:39.885726Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- bootstrap's variance is smaller - it's more stable approach to evaluate model\n- jackknife might give more optimistic evaluation of the model","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n\nax.scatter(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(ridge_jack_metrics['train_mse'], ridge_jack_metrics['val_mse'], ax, n_std=r, edgecolor='red')\n\nax.scatter(ridge_metrics['train_mse'], ridge_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_metrics['train_mse'], ridge_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\n\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\n\nax.legend(['ridge jackknife', 'ridge jackknife', 'ridge bootstrap', 'ridge bootstrap'])\nax.set_title('Bootstrap vs Jackknife for Ridge Regression with alpha=1')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:10:41.139816Z","iopub.execute_input":"2022-09-06T13:10:41.140555Z","iopub.status.idle":"2022-09-06T13:10:41.528935Z","shell.execute_reply.started":"2022-09-06T13:10:41.140493Z","shell.execute_reply":"2022-09-06T13:10:41.527414Z"},"trusted":true},"execution_count":103,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- bootstrap's variance is smaller - it's more stable approach to evaluate model\n- jackknife might give more optimistic evaluation of the model","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(1, 1, figsize=(9, 9))\n\nax.scatter(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], c='red', alpha=0.1)\nconfidence_ellipse(ridge_100_jack_metrics['train_mse'], ridge_100_jack_metrics['val_mse'], ax, n_std=r, edgecolor='red')\n\nax.scatter(ridge_100_metrics['train_mse'], ridge_100_metrics['val_mse'], c='blue', alpha=0.1)\nconfidence_ellipse(ridge_100_metrics['train_mse'], ridge_100_metrics['val_mse'], ax, n_std=r, edgecolor='blue')\n\nax.set_xlabel('train_mse')\nax.set_ylabel('val_mse')\n\nax.legend(['ridge jackknife', 'ridge jackknife', 'ridge bootstrap', 'ridge bootstrap'])\nax.set_title('Bootstrap vs Jackknife for Ridge Regression with alpha=100')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-09-06T13:11:24.685766Z","iopub.execute_input":"2022-09-06T13:11:24.686243Z","iopub.status.idle":"2022-09-06T13:11:25.064463Z","shell.execute_reply.started":"2022-09-06T13:11:24.686193Z","shell.execute_reply":"2022-09-06T13:11:25.063544Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"markdown","source":"Outcome notes:\n- jackknife's ellipsoid is smaller - jackknife might result in more stable results\n- jackknife might give more optimistic evaluation of the model","metadata":{}},{"cell_type":"markdown","source":"TODO: Fit stacked generalization (once again, own implementation based on the module's main book) for both models","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}