{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-07T07:33:39.224722Z","iopub.execute_input":"2022-09-07T07:33:39.225670Z","iopub.status.idle":"2022-09-07T07:33:39.258829Z","shell.execute_reply.started":"2022-09-07T07:33:39.225552Z","shell.execute_reply":"2022-09-07T07:33:39.257469Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor, Pool\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:48:20.362747Z","iopub.execute_input":"2022-09-07T08:48:20.363144Z","iopub.status.idle":"2022-09-07T08:48:20.987885Z","shell.execute_reply.started":"2022-09-07T08:48:20.363085Z","shell.execute_reply":"2022-09-07T08:48:20.986781Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Homework description\n\nTake data from kaggle housing\n1. Take your best prediction\n2. Try pseudo-labeling\n3. Try noisy student approach","metadata":{}},{"cell_type":"markdown","source":"Hypertuned CatBoostRegressor provided best score on Kaggle so far: 0.12548 public score (from Homework #5)\n\ndepth=6, n_estimators=465, learning_rate=0.06\n\nLet's try to create bagging ensemble of 20 CatBoostRegressors with such hyperparameters first and see if the result will be better then currently the best one\n\nNo features will be transformed from numerical to categorical","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-07T07:34:00.734385Z","iopub.execute_input":"2022-09-07T07:34:00.735528Z","iopub.status.idle":"2022-09-07T07:34:00.803885Z","shell.execute_reply.started":"2022-09-07T07:34:00.735475Z","shell.execute_reply":"2022-09-07T07:34:00.802910Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\ncategorical_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j not in [np.int64, np.float64]]\n\ntrain_data[categorical_columns] = train_data[categorical_columns].fillna(\"Other\")\ntrain_data[numeric_columns] = train_data[numeric_columns].fillna(-1)\ntest_data[categorical_columns] = test_data[categorical_columns].fillna(\"Other\")\ntest_data[numeric_columns] = test_data[numeric_columns].fillna(-1)\nx_train = train_data[numeric_columns + categorical_columns]\nx_test = test_data[numeric_columns + categorical_columns]\n\ny_train = np.log(train_data['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:22:53.691789Z","iopub.execute_input":"2022-09-06T16:22:53.692210Z","iopub.status.idle":"2022-09-06T16:22:53.748852Z","shell.execute_reply.started":"2022-09-06T16:22:53.692177Z","shell.execute_reply":"2022-09-06T16:22:53.747601Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"y_pred = []\n\nM = 20\nTEST_SIZE = 0.2\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred.append(model.predict(pool_test)) \n    \ntest_targets = np.exp(np.mean(y_pred, axis=0))\n    \nsubmit_bagging = pd.DataFrame()\nsubmit_bagging['Id'] = test_data['Id']\nsubmit_bagging['SalePrice'] = test_targets\nprint(submit_bagging)\n\nsubmit_bagging.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point2_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T14:25:08.949467Z","iopub.execute_input":"2022-09-06T14:25:08.950014Z","iopub.status.idle":"2022-09-06T14:28:29.364545Z","shell.execute_reply.started":"2022-09-06T14:25:08.949966Z","shell.execute_reply":"2022-09-06T14:28:29.362310Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Above ensemble scored with 0.12297 public score which is better than previous high score.\n\nNow with test size = 0.1\n","metadata":{}},{"cell_type":"code","source":"y_pred2 = []\n\nTEST_SIZE = 0.1\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred2.append(model.predict(pool_test)) \n    \ntest_targets2 = np.exp(np.mean(y_pred2, axis=0))\n    \nsubmit_bagging2 = pd.DataFrame()\nsubmit_bagging2['Id'] = test_data['Id']\nsubmit_bagging2['SalePrice'] = test_targets2\nprint(submit_bagging2)\n\nsubmit_bagging2.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point1_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:10:29.283335Z","iopub.execute_input":"2022-09-06T15:10:29.283756Z","iopub.status.idle":"2022-09-06T15:13:59.150428Z","shell.execute_reply.started":"2022-09-06T15:10:29.283720Z","shell.execute_reply":"2022-09-06T15:13:59.148909Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"Test size = 0.1 helped to achieve even better public score: 0.12252","metadata":{}},{"cell_type":"code","source":"y_pred3 = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred3.append(model.predict(pool_test)) \n    \ntest_targets3 = np.exp(np.mean(y_pred3, axis=0))\n    \nsubmit_bagging3 = pd.DataFrame()\nsubmit_bagging3['Id'] = test_data['Id']\nsubmit_bagging3['SalePrice'] = test_targets3\nprint(submit_bagging3)\n\nsubmit_bagging3.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point05_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:19:20.487106Z","iopub.execute_input":"2022-09-06T15:19:20.487773Z","iopub.status.idle":"2022-09-06T15:22:48.062859Z","shell.execute_reply.started":"2022-09-06T15:19:20.487712Z","shell.execute_reply":"2022-09-06T15:22:48.061611Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"Decreasing test size to 0.05 led to (once again) better public score: 0.12206","metadata":{}},{"cell_type":"markdown","source":"### Pseudo-labeling","metadata":{}},{"cell_type":"code","source":"# Add small noise into test data in a naive way (some of features marked as numeric are indeed categorical + is adding gaussian noise to integer features actually ok?)\n\nmu, sigma = 0, 0.1\nx_test_noise = np.random.normal(mu, sigma, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\nprint(x_test_noise)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:22:58.777841Z","iopub.execute_input":"2022-09-06T16:22:58.778280Z","iopub.status.idle":"2022-09-06T16:22:58.792127Z","shell.execute_reply.started":"2022-09-06T16:22:58.778234Z","shell.execute_reply":"2022-09-06T16:22:58.790430Z"},"trusted":true},"execution_count":58,"outputs":[]},{"cell_type":"code","source":"x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\nx_test_with_noise_only_numeric","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:25:28.211198Z","iopub.execute_input":"2022-09-06T16:25:28.212842Z","iopub.status.idle":"2022-09-06T16:25:28.263787Z","shell.execute_reply.started":"2022-09-06T16:25:28.212781Z","shell.execute_reply":"2022-09-06T16:25:28.262264Z"},"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\nx_test_with_noise","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:30:16.696145Z","iopub.execute_input":"2022-09-06T16:30:16.697453Z","iopub.status.idle":"2022-09-06T16:30:16.731883Z","shell.execute_reply.started":"2022-09-06T16:30:16.697394Z","shell.execute_reply":"2022-09-06T16:30:16.730945Z"},"trusted":true},"execution_count":62,"outputs":[]},{"cell_type":"markdown","source":"Get test's targets for data with noise\n\nApproach #1 - predict new test targets (pseudo-labels) based on features with noise - don't apply noise to target itself","metadata":{}},{"cell_type":"code","source":"y_pred_for_x_test_with_noise = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test_with_noise, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred_for_x_test_with_noise.append(model.predict(pool_test)) \n    \n# important! don't use exponent here\ntest_target_with_noise = np.mean(y_pred_for_x_test_with_noise, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:38:34.839646Z","iopub.execute_input":"2022-09-06T16:38:34.840249Z","iopub.status.idle":"2022-09-06T16:38:34.848426Z","shell.execute_reply.started":"2022-09-06T16:38:34.840200Z","shell.execute_reply":"2022-09-06T16:38:34.847028Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\nx_train_combined","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:36:59.567124Z","iopub.execute_input":"2022-09-06T16:36:59.567550Z","iopub.status.idle":"2022-09-06T16:36:59.610268Z","shell.execute_reply.started":"2022-09-06T16:36:59.567516Z","shell.execute_reply":"2022-09-06T16:36:59.609146Z"},"trusted":true},"execution_count":65,"outputs":[]},{"cell_type":"code","source":"test_target_with_noise = pd.Series(data=test_target_with_noise, name='SalePrice')\ntest_target_with_noise","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:39:30.126026Z","iopub.execute_input":"2022-09-06T16:39:30.126465Z","iopub.status.idle":"2022-09-06T16:39:30.137181Z","shell.execute_reply.started":"2022-09-06T16:39:30.126430Z","shell.execute_reply":"2022-09-06T16:39:30.135842Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"y_train_combined = y_train.append(test_target_with_noise, ignore_index=True)\ny_train_combined","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:39:38.512485Z","iopub.execute_input":"2022-09-06T16:39:38.512895Z","iopub.status.idle":"2022-09-06T16:39:38.523974Z","shell.execute_reply.started":"2022-09-06T16:39:38.512862Z","shell.execute_reply":"2022-09-06T16:39:38.522971Z"},"trusted":true},"execution_count":72,"outputs":[]},{"cell_type":"code","source":"y_pred4 = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train_combined, y_train_combined, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred4.append(model.predict(pool_test)) \n    \ntest_targets4 = np.exp(np.mean(y_pred4, axis=0))\n    \nsubmit_pseudolabeling = pd.DataFrame()\nsubmit_pseudolabeling['Id'] = test_data['Id']\nsubmit_pseudolabeling['SalePrice'] = test_targets4\nprint(submit_pseudolabeling)\n\nsubmit_pseudolabeling.to_csv('/kaggle/working/pseudolabeling2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:43.192848Z","iopub.execute_input":"2022-09-06T17:04:43.193491Z","iopub.status.idle":"2022-09-06T17:04:43.213654Z","shell.execute_reply.started":"2022-09-06T17:04:43.193445Z","shell.execute_reply":"2022-09-06T17:04:43.212088Z"},"trusted":true},"execution_count":81,"outputs":[]},{"cell_type":"markdown","source":"The public score achieved by using pseudo-labeling approach used above: 0.12102\n\nIt is know the best score achieved","metadata":{}},{"cell_type":"markdown","source":"### Noisy Student based on the following paper: https://arxiv.org/pdf/1911.04252.pdf","metadata":{}},{"cell_type":"code","source":"def load_and_preprocess():\n    train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n    test_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n    numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\n    categorical_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j not in [np.int64, np.float64]]\n\n    train_data[categorical_columns] = train_data[categorical_columns].fillna(\"Other\")\n    train_data[numeric_columns] = train_data[numeric_columns].fillna(-1)\n    test_data[categorical_columns] = test_data[categorical_columns].fillna(\"Other\")\n    test_data[numeric_columns] = test_data[numeric_columns].fillna(-1)\n    x_train = train_data[numeric_columns + categorical_columns]\n    x_test = test_data[numeric_columns + categorical_columns]\n\n    y_train = np.log(train_data['SalePrice'])\n    \n    return x_train, y_train, x_test, numeric_columns, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:46:53.595067Z","iopub.execute_input":"2022-09-07T08:46:53.595496Z","iopub.status.idle":"2022-09-07T08:46:53.604576Z","shell.execute_reply.started":"2022-09-07T08:46:53.595461Z","shell.execute_reply":"2022-09-07T08:46:53.603438Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MU, SIGMA = 0, 0.1\nTEST_SIZE = 0.05\nM = 20\n\ndef train_models(x_train, y_train, categorical_columns, print_text):\n    models = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n    print(print_text)\n    for k, model in enumerate(tqdm(models)):\n        x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n        pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n        model.fit(pool_train)\n        \n    return models\n\ndef generate_pseudo_labels(x_test, categorical_columns, models):\n    pseudolabels = []\n    for k, model in enumerate(models):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        pseudolabels.append(model.predict(pool_test)) \n\n    # important! don't use exponent here\n    pseudolabels = np.mean(pseudolabels, axis=0)\n    \n    return np.array(pseudolabels)\n\ndef add_noise(x_test, pseudo_labels):\n    x_test_noise = np.random.normal(MU, SIGMA, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\n    x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\n    x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\n    \n    pseudo_labels_noise = np.random.normal(MU, SIGMA, [pseudo_labels.shape[0]])\n    pseudo_labels_with_noise = pseudo_labels + pseudo_labels_noise\n    \n    return x_test_with_noise, pseudo_labels_with_noise\n\ndef combine_data(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise):\n    x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\n    pseudo_labels_with_noise = pd.Series(data=pseudo_labels_with_noise, name='SalePrice')\n    y_train_combined = y_train.append(pseudo_labels_with_noise, ignore_index=True)\n    \n    return x_train_combined, y_train_combined\n\ndef student_predict(x_test, categorical_columns, models):\n    y_preds = []\n    print('Final prediction...')\n    for k, model in enumerate(tqdm(models)):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        y_preds.append(model.predict(pool_test)) \n\n    y_pred = np.exp(np.mean(y_preds, axis=0))\n    \n    return y_pred\n\ndef noisy_student(iterations):\n    x_train, y_train, x_test, numeric_columns, categorical_columns = load_and_preprocess()\n    teacher_models = train_models(x_train, y_train, categorical_columns, 'Learning on initial train data...')\n    print('Iterative training...')\n    for i in tqdm(range(iterations)):\n        pseudo_labels = generate_pseudo_labels(x_test, categorical_columns, teacher_models)\n        # Approach #2 - Apply noise to both test data and pseudo-labels\n        x_test_with_noise, pseudo_labels_with_noise = add_noise(x_test, pseudo_labels)\n        x_train, y_train = combine_data(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise)\n        student_models = train_models(x_train, y_train, categorical_columns, 'Learning on data with noise...')\n        teacher_models = student_models\n    y_pred = student_predict(x_test, categorical_columns, teacher_models)\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T12:43:14.974493Z","iopub.execute_input":"2022-09-07T12:43:14.974918Z","iopub.status.idle":"2022-09-07T12:43:14.995294Z","shell.execute_reply.started":"2022-09-07T12:43:14.974884Z","shell.execute_reply":"2022-09-07T12:43:14.993932Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"y_pred = noisy_student(10)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:29:53.712070Z","iopub.execute_input":"2022-09-07T09:29:53.712510Z","iopub.status.idle":"2022-09-07T11:17:00.778582Z","shell.execute_reply.started":"2022-09-07T09:29:53.712471Z","shell.execute_reply":"2022-09-07T11:17:00.775197Z"},"trusted":true},"execution_count":48,"outputs":[]},{"cell_type":"code","source":"submit_noisy_student = pd.DataFrame()\nsubmit_noisy_student['Id'] = test_data['Id']\nsubmit_noisy_student['SalePrice'] = y_pred\n\nsubmit_noisy_student.to_csv('/kaggle/working/noisy_student.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T11:20:40.698527Z","iopub.execute_input":"2022-09-07T11:20:40.699377Z","iopub.status.idle":"2022-09-07T11:20:40.728056Z","shell.execute_reply.started":"2022-09-07T11:20:40.699332Z","shell.execute_reply":"2022-09-07T11:20:40.727088Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"Training noisy student for 10 iterations resulted in the public score = 0.12272","metadata":{}},{"cell_type":"code","source":"y_pred_3_iters = noisy_student(3)\nsubmit_noisy_student_3_iters = pd.DataFrame()\nsubmit_noisy_student_3_iters['Id'] = test_data['Id']\nsubmit_noisy_student_3_iters['SalePrice'] = y_pred_3_iters\n\nsubmit_noisy_student_3_iters.to_csv('/kaggle/working/noisy_student_3_iters.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T11:28:54.907329Z","iopub.execute_input":"2022-09-07T11:28:54.907795Z","iopub.status.idle":"2022-09-07T11:51:53.618365Z","shell.execute_reply.started":"2022-09-07T11:28:54.907758Z","shell.execute_reply":"2022-09-07T11:51:53.616945Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"markdown","source":"Applying noisy student approach with 3 iterations resulted in worse public score than for 10 iterations: 0.12450","metadata":{}},{"cell_type":"markdown","source":"Let's try:\n1. scaling features to [0, 1] range and the target with the same type of scaler and then use inverse_transform to bring back log values\n2. apply Gaussian noise with smaller sigma","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nMU, SIGMA = 0, 0.001\n\ndef scale(x_train, x_test, y_train, numeric_columns):\n    scaler = MinMaxScaler().fit(x_train[numeric_columns])\n    x_train[numeric_columns] = scaler.transform(x_train[numeric_columns])\n    x_test[numeric_columns] = scaler.transform(x_test[numeric_columns])\n    target_scaler = MinMaxScaler().fit(y_train.values.reshape(-1, 1))\n    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))\n    \n    return x_train, x_test, y_train, target_scaler\n    \ndef add_noise_scale_version(x_test, pseudo_labels):\n    x_test_noise = np.random.normal(MU, SIGMA, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\n    x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\n    x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\n    \n    pseudo_labels_noise = np.random.normal(MU, SIGMA, [pseudo_labels.shape[0]])\n    pseudo_labels_with_noise = pseudo_labels + pseudo_labels_noise\n    \n    return x_test_with_noise, pseudo_labels_with_noise\n\ndef combine_data_scale_version(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise):\n    x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\n    pseudo_labels_with_noise = pd.Series(data=pseudo_labels_with_noise, name='SalePrice')\n    if type(y_train) != pd.Series:\n        y_train = pd.Series(data=y_train.flatten(), name='SalePrice')\n    y_train_combined = y_train.append(pseudo_labels_with_noise, ignore_index=True)\n    return x_train_combined, y_train_combined\n\ndef student_predict_scale_version(x_test, categorical_columns, models, target_scaler):\n    y_preds = []\n    print('Final prediction...')\n    for k, model in enumerate(tqdm(models)):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        y_preds.append(model.predict(pool_test)) \n\n    y_pred_mean = np.mean(y_preds, axis=0)\n    y_pred_inverse_scaled = target_scaler.inverse_transform(y_pred_mean.reshape(-1, 1))\n    y_pred = np.array(np.exp(y_pred_inverse_scaled).flatten())\n    \n    return y_pred\n\ndef noisy_student_scale_version(iterations):\n    x_train, y_train, x_test, numeric_columns, categorical_columns = load_and_preprocess()\n    x_train, x_test, y_train, target_scaler = scale(x_train, x_test, y_train, numeric_columns)\n    teacher_models = train_models(x_train, y_train, categorical_columns, 'Learning on initial train data...')\n    print('Iterative training...')\n    for i in tqdm(range(iterations)):\n        pseudo_labels = generate_pseudo_labels(x_test, categorical_columns, teacher_models)\n        x_test_with_noise, pseudo_labels_with_noise = add_noise_scale_version(x_test, pseudo_labels)\n        x_train, y_train = combine_data_scale_version(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise)\n        student_models = train_models(x_train, y_train, categorical_columns, 'Learning on data with noise...')\n        teacher_models = student_models\n    y_pred = student_predict_scale_version(x_test, categorical_columns, teacher_models, target_scaler)\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T13:02:43.220653Z","iopub.execute_input":"2022-09-07T13:02:43.221132Z","iopub.status.idle":"2022-09-07T13:02:43.239699Z","shell.execute_reply.started":"2022-09-07T13:02:43.221075Z","shell.execute_reply":"2022-09-07T13:02:43.238742Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"y_pred_3_iters_scaled = noisy_student_scale_version(3)\nsubmit_noisy_student_3_iters_scaled = pd.DataFrame()\nsubmit_noisy_student_3_iters_scaled['Id'] = test_data['Id']\nsubmit_noisy_student_3_iters_scaled['SalePrice'] = y_pred_3_iters_scaled\n\nsubmit_noisy_student_3_iters_scaled.to_csv('/kaggle/working/noisy_student_3_iters_scaled.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T13:02:46.057595Z","iopub.execute_input":"2022-09-07T13:02:46.058335Z","iopub.status.idle":"2022-09-07T13:25:20.175714Z","shell.execute_reply.started":"2022-09-07T13:02:46.058300Z","shell.execute_reply":"2022-09-07T13:25:20.174614Z"},"trusted":true},"execution_count":100,"outputs":[]},{"cell_type":"markdown","source":"Previous best public score was: 0.12102\n\nThis time, with applied scaling, noisy student helped to achieve the new best score: 0.12032\n","metadata":{}}]}