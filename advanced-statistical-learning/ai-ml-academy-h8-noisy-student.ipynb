{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-09-07T07:33:39.224722Z","iopub.execute_input":"2022-09-07T07:33:39.225670Z","iopub.status.idle":"2022-09-07T07:33:39.258829Z","shell.execute_reply.started":"2022-09-07T07:33:39.225552Z","shell.execute_reply":"2022-09-07T07:33:39.257469Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/house-prices-advanced-regression-techniques/sample_submission.csv\n/kaggle/input/house-prices-advanced-regression-techniques/data_description.txt\n/kaggle/input/house-prices-advanced-regression-techniques/train.csv\n/kaggle/input/house-prices-advanced-regression-techniques/test.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom catboost import CatBoostRegressor, Pool\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:48:20.362747Z","iopub.execute_input":"2022-09-07T08:48:20.363144Z","iopub.status.idle":"2022-09-07T08:48:20.987885Z","shell.execute_reply.started":"2022-09-07T08:48:20.363085Z","shell.execute_reply":"2022-09-07T08:48:20.986781Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"# Homework description\n\nTake data from kaggle housing\n1. Take your best prediction\n2. Try pseudo-labeling\n3. Try noisy student approach","metadata":{}},{"cell_type":"markdown","source":"Hypertuned CatBoostRegressor provided best score on Kaggle so far: 0.12548 public score (from Homework #5)\n\ndepth=6, n_estimators=465, learning_rate=0.06\n\nLet's try to create bagging ensemble of 20 CatBoostRegressors with such hyperparameters first and see if the result will be better then currently the best one\n\nNo features will be transformed from numerical to categorical","metadata":{}},{"cell_type":"code","source":"train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\ntest_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-09-07T07:34:00.734385Z","iopub.execute_input":"2022-09-07T07:34:00.735528Z","iopub.status.idle":"2022-09-07T07:34:00.803885Z","shell.execute_reply.started":"2022-09-07T07:34:00.735475Z","shell.execute_reply":"2022-09-07T07:34:00.802910Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\ncategorical_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j not in [np.int64, np.float64]]\n\ntrain_data[categorical_columns] = train_data[categorical_columns].fillna(\"Other\")\ntrain_data[numeric_columns] = train_data[numeric_columns].fillna(-1)\ntest_data[categorical_columns] = test_data[categorical_columns].fillna(\"Other\")\ntest_data[numeric_columns] = test_data[numeric_columns].fillna(-1)\nx_train = train_data[numeric_columns + categorical_columns]\nx_test = test_data[numeric_columns + categorical_columns]\n\ny_train = np.log(train_data['SalePrice'])","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:22:53.691789Z","iopub.execute_input":"2022-09-06T16:22:53.692210Z","iopub.status.idle":"2022-09-06T16:22:53.748852Z","shell.execute_reply.started":"2022-09-06T16:22:53.692177Z","shell.execute_reply":"2022-09-06T16:22:53.747601Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"y_pred = []\n\nM = 20\nTEST_SIZE = 0.2\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred.append(model.predict(pool_test)) \n    \ntest_targets = np.exp(np.mean(y_pred, axis=0))\n    \nsubmit_bagging = pd.DataFrame()\nsubmit_bagging['Id'] = test_data['Id']\nsubmit_bagging['SalePrice'] = test_targets\nprint(submit_bagging)\n\nsubmit_bagging.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point2_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T14:25:08.949467Z","iopub.execute_input":"2022-09-06T14:25:08.950014Z","iopub.status.idle":"2022-09-06T14:28:29.364545Z","shell.execute_reply.started":"2022-09-06T14:25:08.949966Z","shell.execute_reply":"2022-09-06T14:28:29.362310Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [03:20<00:00, 10.02s/it]","output_type":"stream"},{"name":"stdout","text":"        Id      SalePrice\n0     1461  117963.972322\n1     1462  157796.649545\n2     1463  183460.097828\n3     1464  192129.425290\n4     1465  189206.119503\n...    ...            ...\n1454  2915   83811.534474\n1455  2916   83967.171190\n1456  2917  163140.641144\n1457  2918  116515.686215\n1458  2919  219268.905765\n\n[1459 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Above ensemble scored with 0.12297 public score which is better than previous high score.\n\nNow with test size = 0.1\n","metadata":{}},{"cell_type":"code","source":"y_pred2 = []\n\nTEST_SIZE = 0.1\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred2.append(model.predict(pool_test)) \n    \ntest_targets2 = np.exp(np.mean(y_pred2, axis=0))\n    \nsubmit_bagging2 = pd.DataFrame()\nsubmit_bagging2['Id'] = test_data['Id']\nsubmit_bagging2['SalePrice'] = test_targets2\nprint(submit_bagging2)\n\nsubmit_bagging2.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point1_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:10:29.283335Z","iopub.execute_input":"2022-09-06T15:10:29.283756Z","iopub.status.idle":"2022-09-06T15:13:59.150428Z","shell.execute_reply.started":"2022-09-06T15:10:29.283720Z","shell.execute_reply":"2022-09-06T15:13:59.148909Z"},"trusted":true},"execution_count":25,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [03:29<00:00, 10.49s/it]","output_type":"stream"},{"name":"stdout","text":"        Id      SalePrice\n0     1461  117778.201319\n1     1462  157380.532174\n2     1463  183697.289624\n3     1464  191683.811679\n4     1465  188308.457056\n...    ...            ...\n1454  2915   83998.271852\n1455  2916   83818.862894\n1456  2917  163618.786982\n1457  2918  116526.724487\n1458  2919  221166.170050\n\n[1459 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Test size = 0.1 helped to achieve even better public score: 0.12252","metadata":{}},{"cell_type":"code","source":"y_pred3 = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred3.append(model.predict(pool_test)) \n    \ntest_targets3 = np.exp(np.mean(y_pred3, axis=0))\n    \nsubmit_bagging3 = pd.DataFrame()\nsubmit_bagging3['Id'] = test_data['Id']\nsubmit_bagging3['SalePrice'] = test_targets3\nprint(submit_bagging3)\n\nsubmit_bagging3.to_csv('/kaggle/working/hypertuned_20_catboosts_bagging_point05_test_split.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T15:19:20.487106Z","iopub.execute_input":"2022-09-06T15:19:20.487773Z","iopub.status.idle":"2022-09-06T15:22:48.062859Z","shell.execute_reply.started":"2022-09-06T15:19:20.487712Z","shell.execute_reply":"2022-09-06T15:22:48.061611Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stderr","text":"100%|██████████| 20/20 [03:27<00:00, 10.38s/it]","output_type":"stream"},{"name":"stdout","text":"        Id      SalePrice\n0     1461  117322.385497\n1     1462  156434.997498\n2     1463  185267.941341\n3     1464  192854.722216\n4     1465  186536.924132\n...    ...            ...\n1454  2915   84129.796504\n1455  2916   83835.703668\n1456  2917  163518.741341\n1457  2918  115262.171182\n1458  2919  218610.678871\n\n[1459 rows x 2 columns]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Decreasing test size to 0.05 led to (once again) better public score: 0.12206","metadata":{}},{"cell_type":"markdown","source":"### Pseudo-labeling","metadata":{}},{"cell_type":"code","source":"# Add small noise into test data in a naive way (some of features marked as numeric are indeed categorical + is adding gaussian noise to integer features actually ok?)\n\nmu, sigma = 0, 0.1\nx_test_noise = np.random.normal(mu, sigma, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\nprint(x_test_noise)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:22:58.777841Z","iopub.execute_input":"2022-09-06T16:22:58.778280Z","iopub.status.idle":"2022-09-06T16:22:58.792127Z","shell.execute_reply.started":"2022-09-06T16:22:58.778234Z","shell.execute_reply":"2022-09-06T16:22:58.790430Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"[[-0.06842232  0.12753385  0.03372705 ...  0.00083066  0.05923981\n  -0.04419231]\n [-0.14130591 -0.18630972 -0.00788724 ... -0.06577228 -0.01146207\n  -0.05498295]\n [-0.0903799   0.00292425  0.01902806 ... -0.19040073 -0.0036391\n  -0.06787715]\n ...\n [-0.01931702 -0.04084132 -0.0092156  ... -0.10523636  0.10983165\n  -0.11015892]\n [-0.04112879 -0.07369342 -0.05583205 ... -0.20370248  0.05042826\n   0.06755908]\n [-0.09663089  0.00553424 -0.00743605 ... -0.03590264 -0.04623228\n   0.00459231]]\n","output_type":"stream"}]},{"cell_type":"code","source":"x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\nx_test_with_noise_only_numeric","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:25:28.211198Z","iopub.execute_input":"2022-09-06T16:25:28.212842Z","iopub.status.idle":"2022-09-06T16:25:28.263787Z","shell.execute_reply.started":"2022-09-06T16:25:28.212781Z","shell.execute_reply":"2022-09-06T16:25:28.262264Z"},"trusted":true},"execution_count":59,"outputs":[{"execution_count":59,"output_type":"execute_result","data":{"text/plain":"      MSSubClass  LotFrontage       LotArea  OverallQual  OverallCond  \\\n0      19.982531    79.899160  11622.060420     5.104476     5.892204   \n1      19.895882    80.914054  14267.024890     6.196708     6.243939   \n2      59.997782    74.109877  13829.901750     4.980579     4.981760   \n3      60.041995    78.019383   9977.916582     6.039517     5.887540   \n4     120.028970    42.857917   5004.867762     8.060135     5.027041   \n...          ...          ...           ...          ...          ...   \n1454  159.888495    21.006196   1936.094715     3.849836     7.106053   \n1455  160.034686    20.903897   1894.189414     4.136896     4.909377   \n1456   19.967012   160.056096  19999.900762     5.015645     6.910174   \n1457   85.148309    61.990892  10440.873660     4.986973     4.995743   \n1458   59.886365    73.981487   9627.065954     7.128494     4.966657   \n\n        YearBuilt  YearRemodAdd  MasVnrArea   BsmtFinSF1  BsmtFinSF2  ...  \\\n0     1960.982031   1960.899791   -0.068245   468.137332  143.925993  ...   \n1     1958.063501   1958.136205  108.102152   923.124646   -0.168530  ...   \n2     1997.067720   1997.964467    0.003767   791.202736    0.045564  ...   \n3     1998.005386   1998.073876   20.077054   602.034272   -0.082253  ...   \n4     1992.064337   1991.956733    0.147972   262.895629    0.137657  ...   \n...           ...           ...         ...          ...         ...  ...   \n1454  1969.959371   1970.024793    0.176569     0.080942    0.039146  ...   \n1455  1969.905702   1969.869376    0.083226   252.178471   -0.048714  ...   \n1456  1959.990160   1996.182373   -0.063737  1224.191589   -0.217492  ...   \n1457  1992.010435   1992.012120    0.027295   337.012916   -0.058021  ...   \n1458  1992.975729   1994.081073   93.830270   758.095292    0.163675  ...   \n\n      GarageArea  WoodDeckSF  OpenPorchSF  EnclosedPorch  3SsnPorch  \\\n0     730.108305  140.010865    -0.015466      -0.249374   0.141507   \n1     312.104208  392.840573    36.018326       0.011181   0.144343   \n2     482.033281  212.059918    33.998076       0.061241   0.056476   \n3     469.944775  359.979975    35.796310       0.046981  -0.017503   \n4     506.017151    0.013124    82.002617       0.124750   0.180190   \n...          ...         ...          ...            ...        ...   \n1454    0.034546   -0.060355    -0.052504      -0.033531   0.007570   \n1455  285.878617   -0.023740    24.041754      -0.131853  -0.108481   \n1456  576.185789  474.010862     0.107829      -0.024383  -0.097727   \n1457   -0.121104   80.002153    32.021600      -0.025568   0.072406   \n1458  650.187544  190.033756    48.055589       0.130414   0.151577   \n\n      ScreenPorch  PoolArea       MiscVal     MoSold       YrSold  \n0      119.854618  0.123432      0.126994   6.139137  2010.031937  \n1       -0.134929  0.132068  12500.033542   6.169430  2010.180999  \n2       -0.137348 -0.093361      0.100499   2.793589  2009.936158  \n3        0.044244  0.035779      0.081908   5.909417  2010.276533  \n4      143.975725  0.022608      0.045278   0.867247  2009.931632  \n...           ...       ...           ...        ...          ...  \n1454     0.096949 -0.080423     -0.054637   5.882058  2006.192757  \n1455    -0.191348 -0.220076      0.040330   3.855388  2006.113798  \n1456     0.076429 -0.182209      0.069762   9.034610  2005.980332  \n1457     0.071310 -0.129825    699.934691   6.989575  2006.038652  \n1458    -0.118668 -0.093508      0.212371  11.063221  2005.885152  \n\n[1459 rows x 36 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageArea</th>\n      <th>WoodDeckSF</th>\n      <th>OpenPorchSF</th>\n      <th>EnclosedPorch</th>\n      <th>3SsnPorch</th>\n      <th>ScreenPorch</th>\n      <th>PoolArea</th>\n      <th>MiscVal</th>\n      <th>MoSold</th>\n      <th>YrSold</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19.982531</td>\n      <td>79.899160</td>\n      <td>11622.060420</td>\n      <td>5.104476</td>\n      <td>5.892204</td>\n      <td>1960.982031</td>\n      <td>1960.899791</td>\n      <td>-0.068245</td>\n      <td>468.137332</td>\n      <td>143.925993</td>\n      <td>...</td>\n      <td>730.108305</td>\n      <td>140.010865</td>\n      <td>-0.015466</td>\n      <td>-0.249374</td>\n      <td>0.141507</td>\n      <td>119.854618</td>\n      <td>0.123432</td>\n      <td>0.126994</td>\n      <td>6.139137</td>\n      <td>2010.031937</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.895882</td>\n      <td>80.914054</td>\n      <td>14267.024890</td>\n      <td>6.196708</td>\n      <td>6.243939</td>\n      <td>1958.063501</td>\n      <td>1958.136205</td>\n      <td>108.102152</td>\n      <td>923.124646</td>\n      <td>-0.168530</td>\n      <td>...</td>\n      <td>312.104208</td>\n      <td>392.840573</td>\n      <td>36.018326</td>\n      <td>0.011181</td>\n      <td>0.144343</td>\n      <td>-0.134929</td>\n      <td>0.132068</td>\n      <td>12500.033542</td>\n      <td>6.169430</td>\n      <td>2010.180999</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59.997782</td>\n      <td>74.109877</td>\n      <td>13829.901750</td>\n      <td>4.980579</td>\n      <td>4.981760</td>\n      <td>1997.067720</td>\n      <td>1997.964467</td>\n      <td>0.003767</td>\n      <td>791.202736</td>\n      <td>0.045564</td>\n      <td>...</td>\n      <td>482.033281</td>\n      <td>212.059918</td>\n      <td>33.998076</td>\n      <td>0.061241</td>\n      <td>0.056476</td>\n      <td>-0.137348</td>\n      <td>-0.093361</td>\n      <td>0.100499</td>\n      <td>2.793589</td>\n      <td>2009.936158</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60.041995</td>\n      <td>78.019383</td>\n      <td>9977.916582</td>\n      <td>6.039517</td>\n      <td>5.887540</td>\n      <td>1998.005386</td>\n      <td>1998.073876</td>\n      <td>20.077054</td>\n      <td>602.034272</td>\n      <td>-0.082253</td>\n      <td>...</td>\n      <td>469.944775</td>\n      <td>359.979975</td>\n      <td>35.796310</td>\n      <td>0.046981</td>\n      <td>-0.017503</td>\n      <td>0.044244</td>\n      <td>0.035779</td>\n      <td>0.081908</td>\n      <td>5.909417</td>\n      <td>2010.276533</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>120.028970</td>\n      <td>42.857917</td>\n      <td>5004.867762</td>\n      <td>8.060135</td>\n      <td>5.027041</td>\n      <td>1992.064337</td>\n      <td>1991.956733</td>\n      <td>0.147972</td>\n      <td>262.895629</td>\n      <td>0.137657</td>\n      <td>...</td>\n      <td>506.017151</td>\n      <td>0.013124</td>\n      <td>82.002617</td>\n      <td>0.124750</td>\n      <td>0.180190</td>\n      <td>143.975725</td>\n      <td>0.022608</td>\n      <td>0.045278</td>\n      <td>0.867247</td>\n      <td>2009.931632</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>159.888495</td>\n      <td>21.006196</td>\n      <td>1936.094715</td>\n      <td>3.849836</td>\n      <td>7.106053</td>\n      <td>1969.959371</td>\n      <td>1970.024793</td>\n      <td>0.176569</td>\n      <td>0.080942</td>\n      <td>0.039146</td>\n      <td>...</td>\n      <td>0.034546</td>\n      <td>-0.060355</td>\n      <td>-0.052504</td>\n      <td>-0.033531</td>\n      <td>0.007570</td>\n      <td>0.096949</td>\n      <td>-0.080423</td>\n      <td>-0.054637</td>\n      <td>5.882058</td>\n      <td>2006.192757</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>160.034686</td>\n      <td>20.903897</td>\n      <td>1894.189414</td>\n      <td>4.136896</td>\n      <td>4.909377</td>\n      <td>1969.905702</td>\n      <td>1969.869376</td>\n      <td>0.083226</td>\n      <td>252.178471</td>\n      <td>-0.048714</td>\n      <td>...</td>\n      <td>285.878617</td>\n      <td>-0.023740</td>\n      <td>24.041754</td>\n      <td>-0.131853</td>\n      <td>-0.108481</td>\n      <td>-0.191348</td>\n      <td>-0.220076</td>\n      <td>0.040330</td>\n      <td>3.855388</td>\n      <td>2006.113798</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>19.967012</td>\n      <td>160.056096</td>\n      <td>19999.900762</td>\n      <td>5.015645</td>\n      <td>6.910174</td>\n      <td>1959.990160</td>\n      <td>1996.182373</td>\n      <td>-0.063737</td>\n      <td>1224.191589</td>\n      <td>-0.217492</td>\n      <td>...</td>\n      <td>576.185789</td>\n      <td>474.010862</td>\n      <td>0.107829</td>\n      <td>-0.024383</td>\n      <td>-0.097727</td>\n      <td>0.076429</td>\n      <td>-0.182209</td>\n      <td>0.069762</td>\n      <td>9.034610</td>\n      <td>2005.980332</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>85.148309</td>\n      <td>61.990892</td>\n      <td>10440.873660</td>\n      <td>4.986973</td>\n      <td>4.995743</td>\n      <td>1992.010435</td>\n      <td>1992.012120</td>\n      <td>0.027295</td>\n      <td>337.012916</td>\n      <td>-0.058021</td>\n      <td>...</td>\n      <td>-0.121104</td>\n      <td>80.002153</td>\n      <td>32.021600</td>\n      <td>-0.025568</td>\n      <td>0.072406</td>\n      <td>0.071310</td>\n      <td>-0.129825</td>\n      <td>699.934691</td>\n      <td>6.989575</td>\n      <td>2006.038652</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>59.886365</td>\n      <td>73.981487</td>\n      <td>9627.065954</td>\n      <td>7.128494</td>\n      <td>4.966657</td>\n      <td>1992.975729</td>\n      <td>1994.081073</td>\n      <td>93.830270</td>\n      <td>758.095292</td>\n      <td>0.163675</td>\n      <td>...</td>\n      <td>650.187544</td>\n      <td>190.033756</td>\n      <td>48.055589</td>\n      <td>0.130414</td>\n      <td>0.151577</td>\n      <td>-0.118668</td>\n      <td>-0.093508</td>\n      <td>0.212371</td>\n      <td>11.063221</td>\n      <td>2005.885152</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows × 36 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\nx_test_with_noise","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:30:16.696145Z","iopub.execute_input":"2022-09-06T16:30:16.697453Z","iopub.status.idle":"2022-09-06T16:30:16.731883Z","shell.execute_reply.started":"2022-09-06T16:30:16.697394Z","shell.execute_reply":"2022-09-06T16:30:16.730945Z"},"trusted":true},"execution_count":62,"outputs":[{"execution_count":62,"output_type":"execute_result","data":{"text/plain":"      MSSubClass  LotFrontage       LotArea  OverallQual  OverallCond  \\\n0      19.982531    79.899160  11622.060420     5.104476     5.892204   \n1      19.895882    80.914054  14267.024890     6.196708     6.243939   \n2      59.997782    74.109877  13829.901750     4.980579     4.981760   \n3      60.041995    78.019383   9977.916582     6.039517     5.887540   \n4     120.028970    42.857917   5004.867762     8.060135     5.027041   \n...          ...          ...           ...          ...          ...   \n1454  159.888495    21.006196   1936.094715     3.849836     7.106053   \n1455  160.034686    20.903897   1894.189414     4.136896     4.909377   \n1456   19.967012   160.056096  19999.900762     5.015645     6.910174   \n1457   85.148309    61.990892  10440.873660     4.986973     4.995743   \n1458   59.886365    73.981487   9627.065954     7.128494     4.966657   \n\n        YearBuilt  YearRemodAdd  MasVnrArea   BsmtFinSF1  BsmtFinSF2  ...  \\\n0     1960.982031   1960.899791   -0.068245   468.137332  143.925993  ...   \n1     1958.063501   1958.136205  108.102152   923.124646   -0.168530  ...   \n2     1997.067720   1997.964467    0.003767   791.202736    0.045564  ...   \n3     1998.005386   1998.073876   20.077054   602.034272   -0.082253  ...   \n4     1992.064337   1991.956733    0.147972   262.895629    0.137657  ...   \n...           ...           ...         ...          ...         ...  ...   \n1454  1969.959371   1970.024793    0.176569     0.080942    0.039146  ...   \n1455  1969.905702   1969.869376    0.083226   252.178471   -0.048714  ...   \n1456  1959.990160   1996.182373   -0.063737  1224.191589   -0.217492  ...   \n1457  1992.010435   1992.012120    0.027295   337.012916   -0.058021  ...   \n1458  1992.975729   1994.081073   93.830270   758.095292    0.163675  ...   \n\n      GarageType  GarageFinish  GarageQual  GarageCond  PavedDrive  PoolQC  \\\n0         Attchd           Unf          TA          TA           Y   Other   \n1         Attchd           Unf          TA          TA           Y   Other   \n2         Attchd           Fin          TA          TA           Y   Other   \n3         Attchd           Fin          TA          TA           Y   Other   \n4         Attchd           RFn          TA          TA           Y   Other   \n...          ...           ...         ...         ...         ...     ...   \n1454       Other         Other       Other       Other           Y   Other   \n1455     CarPort           Unf          TA          TA           Y   Other   \n1456      Detchd           Unf          TA          TA           Y   Other   \n1457       Other         Other       Other       Other           Y   Other   \n1458      Attchd           Fin          TA          TA           Y   Other   \n\n      Fence  MiscFeature  SaleType  SaleCondition  \n0     MnPrv        Other        WD         Normal  \n1     Other         Gar2        WD         Normal  \n2     MnPrv        Other        WD         Normal  \n3     Other        Other        WD         Normal  \n4     Other        Other        WD         Normal  \n...     ...          ...       ...            ...  \n1454  Other        Other        WD         Normal  \n1455  Other        Other        WD        Abnorml  \n1456  Other        Other        WD        Abnorml  \n1457  MnPrv         Shed        WD         Normal  \n1458  Other        Other        WD         Normal  \n\n[1459 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageType</th>\n      <th>GarageFinish</th>\n      <th>GarageQual</th>\n      <th>GarageCond</th>\n      <th>PavedDrive</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>19.982531</td>\n      <td>79.899160</td>\n      <td>11622.060420</td>\n      <td>5.104476</td>\n      <td>5.892204</td>\n      <td>1960.982031</td>\n      <td>1960.899791</td>\n      <td>-0.068245</td>\n      <td>468.137332</td>\n      <td>143.925993</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>MnPrv</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>19.895882</td>\n      <td>80.914054</td>\n      <td>14267.024890</td>\n      <td>6.196708</td>\n      <td>6.243939</td>\n      <td>1958.063501</td>\n      <td>1958.136205</td>\n      <td>108.102152</td>\n      <td>923.124646</td>\n      <td>-0.168530</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Gar2</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>59.997782</td>\n      <td>74.109877</td>\n      <td>13829.901750</td>\n      <td>4.980579</td>\n      <td>4.981760</td>\n      <td>1997.067720</td>\n      <td>1997.964467</td>\n      <td>0.003767</td>\n      <td>791.202736</td>\n      <td>0.045564</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Fin</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>MnPrv</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>60.041995</td>\n      <td>78.019383</td>\n      <td>9977.916582</td>\n      <td>6.039517</td>\n      <td>5.887540</td>\n      <td>1998.005386</td>\n      <td>1998.073876</td>\n      <td>20.077054</td>\n      <td>602.034272</td>\n      <td>-0.082253</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Fin</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>120.028970</td>\n      <td>42.857917</td>\n      <td>5004.867762</td>\n      <td>8.060135</td>\n      <td>5.027041</td>\n      <td>1992.064337</td>\n      <td>1991.956733</td>\n      <td>0.147972</td>\n      <td>262.895629</td>\n      <td>0.137657</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>RFn</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1454</th>\n      <td>159.888495</td>\n      <td>21.006196</td>\n      <td>1936.094715</td>\n      <td>3.849836</td>\n      <td>7.106053</td>\n      <td>1969.959371</td>\n      <td>1970.024793</td>\n      <td>0.176569</td>\n      <td>0.080942</td>\n      <td>0.039146</td>\n      <td>...</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1455</th>\n      <td>160.034686</td>\n      <td>20.903897</td>\n      <td>1894.189414</td>\n      <td>4.136896</td>\n      <td>4.909377</td>\n      <td>1969.905702</td>\n      <td>1969.869376</td>\n      <td>0.083226</td>\n      <td>252.178471</td>\n      <td>-0.048714</td>\n      <td>...</td>\n      <td>CarPort</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>1456</th>\n      <td>19.967012</td>\n      <td>160.056096</td>\n      <td>19999.900762</td>\n      <td>5.015645</td>\n      <td>6.910174</td>\n      <td>1959.990160</td>\n      <td>1996.182373</td>\n      <td>-0.063737</td>\n      <td>1224.191589</td>\n      <td>-0.217492</td>\n      <td>...</td>\n      <td>Detchd</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>85.148309</td>\n      <td>61.990892</td>\n      <td>10440.873660</td>\n      <td>4.986973</td>\n      <td>4.995743</td>\n      <td>1992.010435</td>\n      <td>1992.012120</td>\n      <td>0.027295</td>\n      <td>337.012916</td>\n      <td>-0.058021</td>\n      <td>...</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>MnPrv</td>\n      <td>Shed</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1458</th>\n      <td>59.886365</td>\n      <td>73.981487</td>\n      <td>9627.065954</td>\n      <td>7.128494</td>\n      <td>4.966657</td>\n      <td>1992.975729</td>\n      <td>1994.081073</td>\n      <td>93.830270</td>\n      <td>758.095292</td>\n      <td>0.163675</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Fin</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>1459 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Get test's targets for data with noise\n\nApproach #1 - predict new test targets (pseudo-labels) based on features with noise - don't apply noise to target itself","metadata":{}},{"cell_type":"code","source":"y_pred_for_x_test_with_noise = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test_with_noise, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred_for_x_test_with_noise.append(model.predict(pool_test)) \n    \n# important! don't use exponent here\ntest_target_with_noise = np.mean(y_pred_for_x_test_with_noise, axis=0)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:38:34.839646Z","iopub.execute_input":"2022-09-06T16:38:34.840249Z","iopub.status.idle":"2022-09-06T16:38:34.848426Z","shell.execute_reply.started":"2022-09-06T16:38:34.840200Z","shell.execute_reply":"2022-09-06T16:38:34.847028Z"},"trusted":true},"execution_count":69,"outputs":[]},{"cell_type":"code","source":"x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\nx_train_combined","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:36:59.567124Z","iopub.execute_input":"2022-09-06T16:36:59.567550Z","iopub.status.idle":"2022-09-06T16:36:59.610268Z","shell.execute_reply.started":"2022-09-06T16:36:59.567516Z","shell.execute_reply":"2022-09-06T16:36:59.609146Z"},"trusted":true},"execution_count":65,"outputs":[{"execution_count":65,"output_type":"execute_result","data":{"text/plain":"      MSSubClass  LotFrontage       LotArea  OverallQual  OverallCond  \\\n0      60.000000    65.000000   8450.000000     7.000000     5.000000   \n1      20.000000    80.000000   9600.000000     6.000000     8.000000   \n2      60.000000    68.000000  11250.000000     7.000000     5.000000   \n3      70.000000    60.000000   9550.000000     7.000000     5.000000   \n4      60.000000    84.000000  14260.000000     8.000000     5.000000   \n...          ...          ...           ...          ...          ...   \n2914  159.888495    21.006196   1936.094715     3.849836     7.106053   \n2915  160.034686    20.903897   1894.189414     4.136896     4.909377   \n2916   19.967012   160.056096  19999.900762     5.015645     6.910174   \n2917   85.148309    61.990892  10440.873660     4.986973     4.995743   \n2918   59.886365    73.981487   9627.065954     7.128494     4.966657   \n\n        YearBuilt  YearRemodAdd  MasVnrArea   BsmtFinSF1  BsmtFinSF2  ...  \\\n0     2003.000000   2003.000000  196.000000   706.000000    0.000000  ...   \n1     1976.000000   1976.000000    0.000000   978.000000    0.000000  ...   \n2     2001.000000   2002.000000  162.000000   486.000000    0.000000  ...   \n3     1915.000000   1970.000000    0.000000   216.000000    0.000000  ...   \n4     2000.000000   2000.000000  350.000000   655.000000    0.000000  ...   \n...           ...           ...         ...          ...         ...  ...   \n2914  1969.959371   1970.024793    0.176569     0.080942    0.039146  ...   \n2915  1969.905702   1969.869376    0.083226   252.178471   -0.048714  ...   \n2916  1959.990160   1996.182373   -0.063737  1224.191589   -0.217492  ...   \n2917  1992.010435   1992.012120    0.027295   337.012916   -0.058021  ...   \n2918  1992.975729   1994.081073   93.830270   758.095292    0.163675  ...   \n\n      GarageType  GarageFinish  GarageQual  GarageCond  PavedDrive  PoolQC  \\\n0         Attchd           RFn          TA          TA           Y   Other   \n1         Attchd           RFn          TA          TA           Y   Other   \n2         Attchd           RFn          TA          TA           Y   Other   \n3         Detchd           Unf          TA          TA           Y   Other   \n4         Attchd           RFn          TA          TA           Y   Other   \n...          ...           ...         ...         ...         ...     ...   \n2914       Other         Other       Other       Other           Y   Other   \n2915     CarPort           Unf          TA          TA           Y   Other   \n2916      Detchd           Unf          TA          TA           Y   Other   \n2917       Other         Other       Other       Other           Y   Other   \n2918      Attchd           Fin          TA          TA           Y   Other   \n\n      Fence  MiscFeature  SaleType  SaleCondition  \n0     Other        Other        WD         Normal  \n1     Other        Other        WD         Normal  \n2     Other        Other        WD         Normal  \n3     Other        Other        WD        Abnorml  \n4     Other        Other        WD         Normal  \n...     ...          ...       ...            ...  \n2914  Other        Other        WD         Normal  \n2915  Other        Other        WD        Abnorml  \n2916  Other        Other        WD        Abnorml  \n2917  MnPrv         Shed        WD         Normal  \n2918  Other        Other        WD         Normal  \n\n[2919 rows x 79 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>MSSubClass</th>\n      <th>LotFrontage</th>\n      <th>LotArea</th>\n      <th>OverallQual</th>\n      <th>OverallCond</th>\n      <th>YearBuilt</th>\n      <th>YearRemodAdd</th>\n      <th>MasVnrArea</th>\n      <th>BsmtFinSF1</th>\n      <th>BsmtFinSF2</th>\n      <th>...</th>\n      <th>GarageType</th>\n      <th>GarageFinish</th>\n      <th>GarageQual</th>\n      <th>GarageCond</th>\n      <th>PavedDrive</th>\n      <th>PoolQC</th>\n      <th>Fence</th>\n      <th>MiscFeature</th>\n      <th>SaleType</th>\n      <th>SaleCondition</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>60.000000</td>\n      <td>65.000000</td>\n      <td>8450.000000</td>\n      <td>7.000000</td>\n      <td>5.000000</td>\n      <td>2003.000000</td>\n      <td>2003.000000</td>\n      <td>196.000000</td>\n      <td>706.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>RFn</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>20.000000</td>\n      <td>80.000000</td>\n      <td>9600.000000</td>\n      <td>6.000000</td>\n      <td>8.000000</td>\n      <td>1976.000000</td>\n      <td>1976.000000</td>\n      <td>0.000000</td>\n      <td>978.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>RFn</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>60.000000</td>\n      <td>68.000000</td>\n      <td>11250.000000</td>\n      <td>7.000000</td>\n      <td>5.000000</td>\n      <td>2001.000000</td>\n      <td>2002.000000</td>\n      <td>162.000000</td>\n      <td>486.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>RFn</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>70.000000</td>\n      <td>60.000000</td>\n      <td>9550.000000</td>\n      <td>7.000000</td>\n      <td>5.000000</td>\n      <td>1915.000000</td>\n      <td>1970.000000</td>\n      <td>0.000000</td>\n      <td>216.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>Detchd</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>60.000000</td>\n      <td>84.000000</td>\n      <td>14260.000000</td>\n      <td>8.000000</td>\n      <td>5.000000</td>\n      <td>2000.000000</td>\n      <td>2000.000000</td>\n      <td>350.000000</td>\n      <td>655.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>RFn</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2914</th>\n      <td>159.888495</td>\n      <td>21.006196</td>\n      <td>1936.094715</td>\n      <td>3.849836</td>\n      <td>7.106053</td>\n      <td>1969.959371</td>\n      <td>1970.024793</td>\n      <td>0.176569</td>\n      <td>0.080942</td>\n      <td>0.039146</td>\n      <td>...</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2915</th>\n      <td>160.034686</td>\n      <td>20.903897</td>\n      <td>1894.189414</td>\n      <td>4.136896</td>\n      <td>4.909377</td>\n      <td>1969.905702</td>\n      <td>1969.869376</td>\n      <td>0.083226</td>\n      <td>252.178471</td>\n      <td>-0.048714</td>\n      <td>...</td>\n      <td>CarPort</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>2916</th>\n      <td>19.967012</td>\n      <td>160.056096</td>\n      <td>19999.900762</td>\n      <td>5.015645</td>\n      <td>6.910174</td>\n      <td>1959.990160</td>\n      <td>1996.182373</td>\n      <td>-0.063737</td>\n      <td>1224.191589</td>\n      <td>-0.217492</td>\n      <td>...</td>\n      <td>Detchd</td>\n      <td>Unf</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Abnorml</td>\n    </tr>\n    <tr>\n      <th>2917</th>\n      <td>85.148309</td>\n      <td>61.990892</td>\n      <td>10440.873660</td>\n      <td>4.986973</td>\n      <td>4.995743</td>\n      <td>1992.010435</td>\n      <td>1992.012120</td>\n      <td>0.027295</td>\n      <td>337.012916</td>\n      <td>-0.058021</td>\n      <td>...</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>MnPrv</td>\n      <td>Shed</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n    <tr>\n      <th>2918</th>\n      <td>59.886365</td>\n      <td>73.981487</td>\n      <td>9627.065954</td>\n      <td>7.128494</td>\n      <td>4.966657</td>\n      <td>1992.975729</td>\n      <td>1994.081073</td>\n      <td>93.830270</td>\n      <td>758.095292</td>\n      <td>0.163675</td>\n      <td>...</td>\n      <td>Attchd</td>\n      <td>Fin</td>\n      <td>TA</td>\n      <td>TA</td>\n      <td>Y</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>Other</td>\n      <td>WD</td>\n      <td>Normal</td>\n    </tr>\n  </tbody>\n</table>\n<p>2919 rows × 79 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"test_target_with_noise = pd.Series(data=test_target_with_noise, name='SalePrice')\ntest_target_with_noise","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:39:30.126026Z","iopub.execute_input":"2022-09-06T16:39:30.126465Z","iopub.status.idle":"2022-09-06T16:39:30.137181Z","shell.execute_reply.started":"2022-09-06T16:39:30.126430Z","shell.execute_reply":"2022-09-06T16:39:30.135842Z"},"trusted":true},"execution_count":71,"outputs":[{"execution_count":71,"output_type":"execute_result","data":{"text/plain":"0       11.672681\n1       11.960642\n2       12.129558\n3       12.169692\n4       12.136384\n          ...    \n1454    11.340116\n1455    11.336614\n1456    12.004683\n1457    11.655062\n1458    12.295048\nName: SalePrice, Length: 1459, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"y_train_combined = y_train.append(test_target_with_noise, ignore_index=True)\ny_train_combined","metadata":{"execution":{"iopub.status.busy":"2022-09-06T16:39:38.512485Z","iopub.execute_input":"2022-09-06T16:39:38.512895Z","iopub.status.idle":"2022-09-06T16:39:38.523974Z","shell.execute_reply.started":"2022-09-06T16:39:38.512862Z","shell.execute_reply":"2022-09-06T16:39:38.522971Z"},"trusted":true},"execution_count":72,"outputs":[{"execution_count":72,"output_type":"execute_result","data":{"text/plain":"0       12.247694\n1       12.109011\n2       12.317167\n3       11.849398\n4       12.429216\n          ...    \n2914    11.340116\n2915    11.336614\n2916    12.004683\n2917    11.655062\n2918    12.295048\nName: SalePrice, Length: 2919, dtype: float64"},"metadata":{}}]},{"cell_type":"code","source":"y_pred4 = []\n\nTEST_SIZE = 0.05\n\nmodels = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n\nfor k, model in enumerate(tqdm(models)):\n    x_tr, _, y_tr, _ = train_test_split(x_train_combined, y_train_combined, test_size=TEST_SIZE, random_state=k)\n    pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n    pool_test = Pool(x_test, cat_features=categorical_columns)\n    model.fit(pool_train)\n    y_pred4.append(model.predict(pool_test)) \n    \ntest_targets4 = np.exp(np.mean(y_pred4, axis=0))\n    \nsubmit_pseudolabeling = pd.DataFrame()\nsubmit_pseudolabeling['Id'] = test_data['Id']\nsubmit_pseudolabeling['SalePrice'] = test_targets4\nprint(submit_pseudolabeling)\n\nsubmit_pseudolabeling.to_csv('/kaggle/working/pseudolabeling2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-06T17:04:43.192848Z","iopub.execute_input":"2022-09-06T17:04:43.193491Z","iopub.status.idle":"2022-09-06T17:04:43.213654Z","shell.execute_reply.started":"2022-09-06T17:04:43.193445Z","shell.execute_reply":"2022-09-06T17:04:43.212088Z"},"trusted":true},"execution_count":81,"outputs":[{"name":"stdout","text":"        Id      SalePrice\n0     1461  118602.471575\n1     1462  155363.068769\n2     1463  186272.977740\n3     1464  191085.684844\n4     1465  186028.473926\n...    ...            ...\n1454  2915   83900.567863\n1455  2916   83451.048089\n1456  2917  164943.857252\n1457  2918  117182.517799\n1458  2919  218696.336499\n\n[1459 rows x 2 columns]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"The public score achieved by using pseudo-labeling approach used above: 0.12102\n\nIt is know the best score achieved","metadata":{}},{"cell_type":"markdown","source":"### Noisy Student based on the following paper: https://arxiv.org/pdf/1911.04252.pdf","metadata":{}},{"cell_type":"code","source":"def load_and_preprocess():\n    train_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/train.csv\")\n    test_data = pd.read_csv(\"/kaggle/input/house-prices-advanced-regression-techniques/test.csv\")\n\n    numeric_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j in [np.int64, np.float64] and i not in ['SalePrice', 'Id']]\n    categorical_columns = [i for i, j in zip(train_data.columns, train_data.dtypes) if j not in [np.int64, np.float64]]\n\n    train_data[categorical_columns] = train_data[categorical_columns].fillna(\"Other\")\n    train_data[numeric_columns] = train_data[numeric_columns].fillna(-1)\n    test_data[categorical_columns] = test_data[categorical_columns].fillna(\"Other\")\n    test_data[numeric_columns] = test_data[numeric_columns].fillna(-1)\n    x_train = train_data[numeric_columns + categorical_columns]\n    x_test = test_data[numeric_columns + categorical_columns]\n\n    y_train = np.log(train_data['SalePrice'])\n    \n    return x_train, y_train, x_test, numeric_columns, categorical_columns","metadata":{"execution":{"iopub.status.busy":"2022-09-07T08:46:53.595067Z","iopub.execute_input":"2022-09-07T08:46:53.595496Z","iopub.status.idle":"2022-09-07T08:46:53.604576Z","shell.execute_reply.started":"2022-09-07T08:46:53.595461Z","shell.execute_reply":"2022-09-07T08:46:53.603438Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MU, SIGMA = 0, 0.1\nTEST_SIZE = 0.05\nM = 20\n\ndef train_models(x_train, y_train, categorical_columns, print_text):\n    models = [CatBoostRegressor(verbose=False, depth=6, n_estimators=465, learning_rate=0.06) for i in range(M)]\n    print(print_text)\n    for k, model in enumerate(tqdm(models)):\n        x_tr, _, y_tr, _ = train_test_split(x_train, y_train, test_size=TEST_SIZE, random_state=k)\n        pool_train = Pool(x_tr, y_tr, cat_features=categorical_columns)\n        model.fit(pool_train)\n        \n    return models\n\ndef generate_pseudo_labels(x_test, categorical_columns, models):\n    pseudolabels = []\n    for k, model in enumerate(models):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        pseudolabels.append(model.predict(pool_test)) \n\n    # important! don't use exponent here\n    pseudolabels = np.mean(pseudolabels, axis=0)\n    \n    return np.array(pseudolabels)\n\ndef add_noise(x_test, pseudo_labels):\n    x_test_noise = np.random.normal(MU, SIGMA, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\n    x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\n    x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\n    \n    pseudo_labels_noise = np.random.normal(MU, SIGMA, [pseudo_labels.shape[0]])\n    pseudo_labels_with_noise = pseudo_labels + pseudo_labels_noise\n    \n    return x_test_with_noise, pseudo_labels_with_noise\n\ndef combine_data(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise):\n    x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\n    pseudo_labels_with_noise = pd.Series(data=pseudo_labels_with_noise, name='SalePrice')\n    y_train_combined = y_train.append(pseudo_labels_with_noise, ignore_index=True)\n    \n    return x_train_combined, y_train_combined\n\ndef student_predict(x_test, categorical_columns, models):\n    y_preds = []\n    print('Final prediction...')\n    for k, model in enumerate(tqdm(models)):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        y_preds.append(model.predict(pool_test)) \n\n    y_pred = np.exp(np.mean(y_preds, axis=0))\n    \n    return y_pred\n\ndef noisy_student(iterations):\n    x_train, y_train, x_test, numeric_columns, categorical_columns = load_and_preprocess()\n    teacher_models = train_models(x_train, y_train, categorical_columns, 'Learning on initial train data...')\n    print('Iterative training...')\n    for i in tqdm(range(iterations)):\n        pseudo_labels = generate_pseudo_labels(x_test, categorical_columns, teacher_models)\n        # Approach #2 - Apply noise to both test data and pseudo-labels\n        x_test_with_noise, pseudo_labels_with_noise = add_noise(x_test, pseudo_labels)\n        x_train, y_train = combine_data(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise)\n        student_models = train_models(x_train, y_train, categorical_columns, 'Learning on data with noise...')\n        teacher_models = student_models\n    y_pred = student_predict(x_test, categorical_columns, teacher_models)\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T12:43:14.974493Z","iopub.execute_input":"2022-09-07T12:43:14.974918Z","iopub.status.idle":"2022-09-07T12:43:14.995294Z","shell.execute_reply.started":"2022-09-07T12:43:14.974884Z","shell.execute_reply":"2022-09-07T12:43:14.993932Z"},"trusted":true},"execution_count":91,"outputs":[]},{"cell_type":"code","source":"y_pred = noisy_student(10)\ny_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T09:29:53.712070Z","iopub.execute_input":"2022-09-07T09:29:53.712510Z","iopub.status.idle":"2022-09-07T11:17:00.778582Z","shell.execute_reply.started":"2022-09-07T09:29:53.712471Z","shell.execute_reply":"2022-09-07T11:17:00.775197Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Learning on initial train data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [03:05<00:00,  9.28s/it]\n","output_type":"stream"},{"name":"stdout","text":"Iterative training...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/10 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:15<04:58, 15.72s/it]\u001b[A\n 10%|█         | 2/20 [00:31<04:44, 15.83s/it]\u001b[A\n 15%|█▌        | 3/20 [00:47<04:29, 15.85s/it]\u001b[A\n 20%|██        | 4/20 [01:03<04:11, 15.74s/it]\u001b[A\n 25%|██▌       | 5/20 [01:19<03:57, 15.83s/it]\u001b[A\n 30%|███       | 6/20 [01:34<03:40, 15.76s/it]\u001b[A\n 35%|███▌      | 7/20 [01:50<03:25, 15.78s/it]\u001b[A\n 40%|████      | 8/20 [02:06<03:08, 15.69s/it]\u001b[A\n 45%|████▌     | 9/20 [02:21<02:53, 15.73s/it]\u001b[A\n 50%|█████     | 10/20 [02:37<02:36, 15.69s/it]\u001b[A\n 55%|█████▌    | 11/20 [02:53<02:21, 15.77s/it]\u001b[A\n 60%|██████    | 12/20 [03:08<02:05, 15.70s/it]\u001b[A\n 65%|██████▌   | 13/20 [03:24<01:49, 15.70s/it]\u001b[A\n 70%|███████   | 14/20 [03:40<01:34, 15.74s/it]\u001b[A\n 75%|███████▌  | 15/20 [03:55<01:18, 15.61s/it]\u001b[A\n 80%|████████  | 16/20 [04:11<01:02, 15.63s/it]\u001b[A\n 85%|████████▌ | 17/20 [04:27<00:46, 15.66s/it]\u001b[A\n 90%|█████████ | 18/20 [04:43<00:31, 15.73s/it]\u001b[A\n 95%|█████████▌| 19/20 [04:58<00:15, 15.66s/it]\u001b[A\n100%|██████████| 20/20 [05:14<00:00, 15.72s/it]\u001b[A\n 10%|█         | 1/10 [05:14<47:12, 314.72s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:19<06:02, 19.08s/it]\u001b[A\n 10%|█         | 2/20 [00:37<05:38, 18.83s/it]\u001b[A\n 15%|█▌        | 3/20 [00:56<05:20, 18.86s/it]\u001b[A\n 20%|██        | 4/20 [01:15<05:01, 18.85s/it]\u001b[A\n 25%|██▌       | 5/20 [01:34<04:44, 18.96s/it]\u001b[A\n 30%|███       | 6/20 [01:53<04:26, 19.04s/it]\u001b[A\n 35%|███▌      | 7/20 [02:12<04:06, 18.95s/it]\u001b[A\n 40%|████      | 8/20 [02:31<03:48, 19.05s/it]\u001b[A\n 45%|████▌     | 9/20 [02:50<03:28, 18.94s/it]\u001b[A\n 50%|█████     | 10/20 [03:10<03:11, 19.11s/it]\u001b[A\n 55%|█████▌    | 11/20 [03:29<02:51, 19.11s/it]\u001b[A\n 60%|██████    | 12/20 [03:47<02:32, 19.01s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:07<02:14, 19.15s/it]\u001b[A\n 70%|███████   | 14/20 [04:26<01:54, 19.12s/it]\u001b[A\n 75%|███████▌  | 15/20 [04:46<01:36, 19.27s/it]\u001b[A\n 80%|████████  | 16/20 [05:05<01:17, 19.32s/it]\u001b[A\n 85%|████████▌ | 17/20 [05:24<00:57, 19.31s/it]\u001b[A\n 90%|█████████ | 18/20 [05:43<00:38, 19.18s/it]\u001b[A\n 95%|█████████▌| 19/20 [06:02<00:19, 19.22s/it]\u001b[A\n100%|██████████| 20/20 [06:22<00:00, 19.12s/it]\u001b[A\n 20%|██        | 2/10 [11:37<47:18, 354.78s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:22<07:02, 22.22s/it]\u001b[A\n 10%|█         | 2/20 [00:44<06:40, 22.23s/it]\u001b[A\n 15%|█▌        | 3/20 [01:06<06:20, 22.36s/it]\u001b[A\n 20%|██        | 4/20 [01:29<05:58, 22.41s/it]\u001b[A\n 25%|██▌       | 5/20 [01:52<05:38, 22.57s/it]\u001b[A\n 30%|███       | 6/20 [02:14<05:13, 22.41s/it]\u001b[A\n 35%|███▌      | 7/20 [02:36<04:51, 22.46s/it]\u001b[A\n 40%|████      | 8/20 [02:59<04:29, 22.46s/it]\u001b[A\n 45%|████▌     | 9/20 [03:21<04:06, 22.37s/it]\u001b[A\n 50%|█████     | 10/20 [03:43<03:42, 22.26s/it]\u001b[A\n 55%|█████▌    | 11/20 [04:06<03:21, 22.34s/it]\u001b[A\n 60%|██████    | 12/20 [04:28<02:57, 22.21s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:50<02:36, 22.34s/it]\u001b[A\n 70%|███████   | 14/20 [05:13<02:14, 22.36s/it]\u001b[A\n 75%|███████▌  | 15/20 [05:35<01:51, 22.33s/it]\u001b[A\n 80%|████████  | 16/20 [05:57<01:28, 22.23s/it]\u001b[A\n 85%|████████▌ | 17/20 [06:19<01:06, 22.28s/it]\u001b[A\n 90%|█████████ | 18/20 [06:42<00:44, 22.35s/it]\u001b[A\n 95%|█████████▌| 19/20 [07:04<00:22, 22.29s/it]\u001b[A\n100%|██████████| 20/20 [07:26<00:00, 22.35s/it]\u001b[A\n 30%|███       | 3/10 [19:04<46:19, 397.05s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:25<07:56, 25.06s/it]\u001b[A\n 10%|█         | 2/20 [00:50<07:35, 25.33s/it]\u001b[A\n 15%|█▌        | 3/20 [01:16<07:13, 25.50s/it]\u001b[A\n 20%|██        | 4/20 [01:41<06:45, 25.35s/it]\u001b[A\n 25%|██▌       | 5/20 [02:06<06:19, 25.32s/it]\u001b[A\n 30%|███       | 6/20 [02:31<05:52, 25.17s/it]\u001b[A\n 35%|███▌      | 7/20 [02:57<05:28, 25.27s/it]\u001b[A\n 40%|████      | 8/20 [03:22<05:03, 25.26s/it]\u001b[A\n 45%|████▌     | 9/20 [03:47<04:38, 25.35s/it]\u001b[A\n 50%|█████     | 10/20 [04:12<04:12, 25.29s/it]\u001b[A\n 55%|█████▌    | 11/20 [04:38<03:48, 25.35s/it]\u001b[A\n 60%|██████    | 12/20 [05:04<03:23, 25.45s/it]\u001b[A\n 65%|██████▌   | 13/20 [05:38<03:16, 28.07s/it]\u001b[A\n 70%|███████   | 14/20 [06:04<02:44, 27.42s/it]\u001b[A\n 75%|███████▌  | 15/20 [06:30<02:16, 27.23s/it]\u001b[A\n 80%|████████  | 16/20 [06:57<01:47, 26.92s/it]\u001b[A\n 85%|████████▌ | 17/20 [07:22<01:19, 26.54s/it]\u001b[A\n 90%|█████████ | 18/20 [07:48<00:52, 26.39s/it]\u001b[A\n 95%|█████████▌| 19/20 [08:14<00:26, 26.27s/it]\u001b[A\n100%|██████████| 20/20 [08:40<00:00, 26.04s/it]\u001b[A\n 40%|████      | 4/10 [27:46<44:36, 446.05s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:28<09:08, 28.86s/it]\u001b[A\n 10%|█         | 2/20 [00:57<08:42, 29.01s/it]\u001b[A\n 15%|█▌        | 3/20 [01:27<08:14, 29.06s/it]\u001b[A\n 20%|██        | 4/20 [01:56<07:44, 29.05s/it]\u001b[A\n 25%|██▌       | 5/20 [02:25<07:18, 29.23s/it]\u001b[A\n 30%|███       | 6/20 [02:55<06:50, 29.31s/it]\u001b[A\n 35%|███▌      | 7/20 [03:24<06:20, 29.26s/it]\u001b[A\n 40%|████      | 8/20 [03:53<05:51, 29.28s/it]\u001b[A\n 45%|████▌     | 9/20 [04:22<05:21, 29.26s/it]\u001b[A\n 50%|█████     | 10/20 [04:52<04:53, 29.39s/it]\u001b[A\n 55%|█████▌    | 11/20 [05:21<04:24, 29.38s/it]\u001b[A\n 60%|██████    | 12/20 [05:51<03:54, 29.36s/it]\u001b[A\n 65%|██████▌   | 13/20 [06:20<03:25, 29.34s/it]\u001b[A\n 70%|███████   | 14/20 [06:49<02:56, 29.33s/it]\u001b[A\n 75%|███████▌  | 15/20 [07:25<02:36, 31.33s/it]\u001b[A\n 80%|████████  | 16/20 [07:55<02:02, 30.70s/it]\u001b[A\n 85%|████████▌ | 17/20 [08:24<01:30, 30.28s/it]\u001b[A\n 90%|█████████ | 18/20 [08:53<01:00, 30.06s/it]\u001b[A\n 95%|█████████▌| 19/20 [09:23<00:29, 29.82s/it]\u001b[A\n100%|██████████| 20/20 [09:52<00:00, 29.64s/it]\u001b[A\n 50%|█████     | 5/10 [37:39<41:35, 499.17s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:32<10:16, 32.44s/it]\u001b[A\n 10%|█         | 2/20 [01:06<09:57, 33.18s/it]\u001b[A\n 15%|█▌        | 3/20 [01:39<09:25, 33.27s/it]\u001b[A\n 20%|██        | 4/20 [02:12<08:51, 33.22s/it]\u001b[A\n 25%|██▌       | 5/20 [02:44<08:13, 32.90s/it]\u001b[A\n 30%|███       | 6/20 [03:17<07:39, 32.83s/it]\u001b[A\n 35%|███▌      | 7/20 [03:49<07:04, 32.64s/it]\u001b[A\n 40%|████      | 8/20 [04:23<06:36, 33.07s/it]\u001b[A\n 45%|████▌     | 9/20 [04:56<06:01, 32.83s/it]\u001b[A\n 50%|█████     | 10/20 [05:28<05:26, 32.63s/it]\u001b[A\n 55%|█████▌    | 11/20 [06:00<04:52, 32.54s/it]\u001b[A\n 60%|██████    | 12/20 [06:33<04:19, 32.46s/it]\u001b[A\n 65%|██████▌   | 13/20 [07:05<03:47, 32.52s/it]\u001b[A\n 70%|███████   | 14/20 [07:46<03:30, 35.07s/it]\u001b[A\n 75%|███████▌  | 15/20 [08:19<02:51, 34.36s/it]\u001b[A\n 80%|████████  | 16/20 [08:52<02:15, 33.85s/it]\u001b[A\n 85%|████████▌ | 17/20 [09:24<01:40, 33.49s/it]\u001b[A\n 90%|█████████ | 18/20 [09:57<01:06, 33.27s/it]\u001b[A\n 95%|█████████▌| 19/20 [10:29<00:33, 33.04s/it]\u001b[A\n100%|██████████| 20/20 [11:02<00:00, 33.12s/it]\u001b[A\n 60%|██████    | 6/10 [48:42<36:59, 554.82s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:35<11:19, 35.78s/it]\u001b[A\n 10%|█         | 2/20 [01:11<10:44, 35.83s/it]\u001b[A\n 15%|█▌        | 3/20 [01:47<10:11, 35.97s/it]\u001b[A\n 20%|██        | 4/20 [02:23<09:36, 36.00s/it]\u001b[A\n 25%|██▌       | 5/20 [02:59<09:00, 36.01s/it]\u001b[A\n 30%|███       | 6/20 [03:35<08:23, 35.95s/it]\u001b[A\n 35%|███▌      | 7/20 [04:11<07:47, 35.96s/it]\u001b[A\n 40%|████      | 8/20 [04:47<07:10, 35.86s/it]\u001b[A\n 45%|████▌     | 9/20 [05:23<06:35, 35.93s/it]\u001b[A\n 50%|█████     | 10/20 [05:59<06:00, 36.01s/it]\u001b[A\n 55%|█████▌    | 11/20 [06:40<05:38, 37.56s/it]\u001b[A\n 60%|██████    | 12/20 [07:17<04:59, 37.40s/it]\u001b[A\n 65%|██████▌   | 13/20 [07:53<04:19, 37.05s/it]\u001b[A\n 70%|███████   | 14/20 [08:30<03:41, 36.90s/it]\u001b[A\n 75%|███████▌  | 15/20 [09:07<03:04, 36.95s/it]\u001b[A\n 80%|████████  | 16/20 [09:44<02:27, 36.93s/it]\u001b[A\n 85%|████████▌ | 17/20 [10:21<01:50, 36.88s/it]\u001b[A\n 90%|█████████ | 18/20 [10:57<01:13, 36.68s/it]\u001b[A\n 95%|█████████▌| 19/20 [11:34<00:36, 36.94s/it]\u001b[A\n100%|██████████| 20/20 [12:11<00:00, 36.58s/it]\u001b[A\n 70%|███████   | 7/10 [1:00:54<30:38, 612.71s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:39<12:34, 39.68s/it]\u001b[A\n 10%|█         | 2/20 [01:19<11:54, 39.71s/it]\u001b[A\n 15%|█▌        | 3/20 [01:59<11:16, 39.77s/it]\u001b[A\n 20%|██        | 4/20 [02:38<10:34, 39.67s/it]\u001b[A\n 25%|██▌       | 5/20 [03:17<09:49, 39.30s/it]\u001b[A\n 30%|███       | 6/20 [03:56<09:11, 39.39s/it]\u001b[A\n 35%|███▌      | 7/20 [04:39<08:46, 40.48s/it]\u001b[A\n 40%|████      | 8/20 [05:22<08:15, 41.32s/it]\u001b[A\n 45%|████▌     | 9/20 [06:02<07:29, 40.86s/it]\u001b[A\n 50%|█████     | 10/20 [06:42<06:45, 40.51s/it]\u001b[A\n 55%|█████▌    | 11/20 [07:21<06:01, 40.20s/it]\u001b[A\n 60%|██████    | 12/20 [08:01<05:19, 40.00s/it]\u001b[A\n 65%|██████▌   | 13/20 [08:40<04:38, 39.73s/it]\u001b[A\n 70%|███████   | 14/20 [09:20<03:58, 39.71s/it]\u001b[A\n 75%|███████▌  | 15/20 [09:59<03:17, 39.60s/it]\u001b[A\n 80%|████████  | 16/20 [10:39<02:38, 39.67s/it]\u001b[A\n 85%|████████▌ | 17/20 [11:18<01:58, 39.48s/it]\u001b[A\n 90%|█████████ | 18/20 [11:57<01:18, 39.25s/it]\u001b[A\n 95%|█████████▌| 19/20 [12:36<00:39, 39.27s/it]\u001b[A\n100%|██████████| 20/20 [13:16<00:00, 39.83s/it]\u001b[A\n 80%|████████  | 8/10 [1:14:11<22:22, 671.37s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:44<13:59, 44.17s/it]\u001b[A\n 10%|█         | 2/20 [01:26<12:55, 43.10s/it]\u001b[A\n 15%|█▌        | 3/20 [02:15<12:58, 45.78s/it]\u001b[A\n 20%|██        | 4/20 [02:58<11:53, 44.57s/it]\u001b[A\n 25%|██▌       | 5/20 [03:40<10:58, 43.92s/it]\u001b[A\n 30%|███       | 6/20 [04:23<10:07, 43.40s/it]\u001b[A\n 35%|███▌      | 7/20 [05:05<09:20, 43.11s/it]\u001b[A\n 40%|████      | 8/20 [05:48<08:33, 42.83s/it]\u001b[A\n 45%|████▌     | 9/20 [06:29<07:46, 42.44s/it]\u001b[A\n 50%|█████     | 10/20 [07:12<07:04, 42.43s/it]\u001b[A\n 55%|█████▌    | 11/20 [07:54<06:21, 42.36s/it]\u001b[A\n 60%|██████    | 12/20 [08:36<05:38, 42.35s/it]\u001b[A\n 65%|██████▌   | 13/20 [09:19<04:56, 42.39s/it]\u001b[A\n 70%|███████   | 14/20 [10:01<04:14, 42.36s/it]\u001b[A\n 75%|███████▌  | 15/20 [10:44<03:32, 42.48s/it]\u001b[A\n 80%|████████  | 16/20 [11:26<02:49, 42.43s/it]\u001b[A\n 85%|████████▌ | 17/20 [12:15<02:13, 44.34s/it]\u001b[A\n 90%|█████████ | 18/20 [12:58<01:27, 43.91s/it]\u001b[A\n 95%|█████████▌| 19/20 [13:40<00:43, 43.50s/it]\u001b[A\n100%|██████████| 20/20 [14:23<00:00, 43.16s/it]\u001b[A\n 90%|█████████ | 9/10 [1:28:34<12:11, 731.47s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:45<14:22, 45.39s/it]\u001b[A\n 10%|█         | 2/20 [01:31<13:40, 45.60s/it]\u001b[A\n 15%|█▌        | 3/20 [02:17<13:00, 45.90s/it]\u001b[A\n 20%|██        | 4/20 [03:03<12:15, 45.94s/it]\u001b[A\n 25%|██▌       | 5/20 [03:48<11:22, 45.48s/it]\u001b[A\n 30%|███       | 6/20 [04:34<10:40, 45.72s/it]\u001b[A\n 35%|███▌      | 7/20 [05:20<09:54, 45.77s/it]\u001b[A\n 40%|████      | 8/20 [06:05<09:09, 45.76s/it]\u001b[A\n 45%|████▌     | 9/20 [06:51<08:22, 45.67s/it]\u001b[A\n 50%|█████     | 10/20 [07:38<07:40, 46.02s/it]\u001b[A\n 55%|█████▌    | 11/20 [08:30<07:10, 47.86s/it]\u001b[A\n 60%|██████    | 12/20 [09:16<06:19, 47.43s/it]\u001b[A\n 65%|██████▌   | 13/20 [10:02<05:28, 46.95s/it]\u001b[A\n 70%|███████   | 14/20 [10:48<04:40, 46.74s/it]\u001b[A\n 75%|███████▌  | 15/20 [11:35<03:53, 46.63s/it]\u001b[A\n 80%|████████  | 16/20 [12:21<03:06, 46.62s/it]\u001b[A\n 85%|████████▌ | 17/20 [13:07<02:19, 46.40s/it]\u001b[A\n 90%|█████████ | 18/20 [13:53<01:32, 46.22s/it]\u001b[A\n 95%|█████████▌| 19/20 [14:39<00:46, 46.16s/it]\u001b[A\n100%|██████████| 20/20 [15:25<00:00, 46.29s/it]\u001b[A\n100%|██████████| 10/10 [1:44:00<00:00, 624.09s/it]\n","output_type":"stream"},{"name":"stdout","text":"Final prediction...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:00<00:00, 62.31it/s]\n","output_type":"stream"},{"execution_count":48,"output_type":"execute_result","data":{"text/plain":"array([121639.93643978, 158096.87800949, 179482.89102149, ...,\n       165988.88233308, 119944.79208449, 213709.33588139])"},"metadata":{}}]},{"cell_type":"code","source":"submit_noisy_student = pd.DataFrame()\nsubmit_noisy_student['Id'] = test_data['Id']\nsubmit_noisy_student['SalePrice'] = y_pred\n\nsubmit_noisy_student.to_csv('/kaggle/working/noisy_student.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T11:20:40.698527Z","iopub.execute_input":"2022-09-07T11:20:40.699377Z","iopub.status.idle":"2022-09-07T11:20:40.728056Z","shell.execute_reply.started":"2022-09-07T11:20:40.699332Z","shell.execute_reply":"2022-09-07T11:20:40.727088Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"markdown","source":"Training noisy student for 10 iterations resulted in the public score = 0.12272","metadata":{}},{"cell_type":"code","source":"y_pred_3_iters = noisy_student(3)\nsubmit_noisy_student_3_iters = pd.DataFrame()\nsubmit_noisy_student_3_iters['Id'] = test_data['Id']\nsubmit_noisy_student_3_iters['SalePrice'] = y_pred_3_iters\n\nsubmit_noisy_student_3_iters.to_csv('/kaggle/working/noisy_student_3_iters.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T11:28:54.907329Z","iopub.execute_input":"2022-09-07T11:28:54.907795Z","iopub.status.idle":"2022-09-07T11:51:53.618365Z","shell.execute_reply.started":"2022-09-07T11:28:54.907758Z","shell.execute_reply":"2022-09-07T11:51:53.616945Z"},"trusted":true},"execution_count":50,"outputs":[{"name":"stdout","text":"Learning on initial train data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [03:10<00:00,  9.53s/it]\n","output_type":"stream"},{"name":"stdout","text":"Iterative training...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:16<05:06, 16.11s/it]\u001b[A\n 10%|█         | 2/20 [00:32<04:50, 16.12s/it]\u001b[A\n 15%|█▌        | 3/20 [00:48<04:36, 16.29s/it]\u001b[A\n 20%|██        | 4/20 [01:04<04:20, 16.26s/it]\u001b[A\n 25%|██▌       | 5/20 [01:21<04:05, 16.34s/it]\u001b[A\n 30%|███       | 6/20 [01:37<03:46, 16.17s/it]\u001b[A\n 35%|███▌      | 7/20 [01:53<03:32, 16.35s/it]\u001b[A\n 40%|████      | 8/20 [02:10<03:16, 16.38s/it]\u001b[A\n 45%|████▌     | 9/20 [02:27<03:00, 16.45s/it]\u001b[A\n 50%|█████     | 10/20 [02:43<02:43, 16.38s/it]\u001b[A\n 55%|█████▌    | 11/20 [02:59<02:28, 16.48s/it]\u001b[A\n 60%|██████    | 12/20 [03:16<02:11, 16.40s/it]\u001b[A\n 65%|██████▌   | 13/20 [03:33<01:57, 16.81s/it]\u001b[A\n 70%|███████   | 14/20 [03:51<01:41, 16.91s/it]\u001b[A\n 75%|███████▌  | 15/20 [04:07<01:23, 16.64s/it]\u001b[A\n 80%|████████  | 16/20 [04:23<01:06, 16.65s/it]\u001b[A\n 85%|████████▌ | 17/20 [04:40<00:49, 16.63s/it]\u001b[A\n 90%|█████████ | 18/20 [04:57<00:33, 16.77s/it]\u001b[A\n 95%|█████████▌| 19/20 [05:13<00:16, 16.66s/it]\u001b[A\n100%|██████████| 20/20 [05:30<00:00, 16.51s/it]\u001b[A\n 33%|███▎      | 1/3 [05:30<11:01, 330.65s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:19<06:13, 19.68s/it]\u001b[A\n 10%|█         | 2/20 [00:39<05:57, 19.89s/it]\u001b[A\n 15%|█▌        | 3/20 [00:59<05:36, 19.80s/it]\u001b[A\n 20%|██        | 4/20 [01:18<05:15, 19.69s/it]\u001b[A\n 25%|██▌       | 5/20 [01:38<04:55, 19.68s/it]\u001b[A\n 30%|███       | 6/20 [01:57<04:33, 19.55s/it]\u001b[A\n 35%|███▌      | 7/20 [02:18<04:17, 19.82s/it]\u001b[A\n 40%|████      | 8/20 [02:38<03:58, 19.87s/it]\u001b[A\n 45%|████▌     | 9/20 [02:57<03:37, 19.81s/it]\u001b[A\n 50%|█████     | 10/20 [03:18<03:20, 20.02s/it]\u001b[A\n 55%|█████▌    | 11/20 [03:38<02:59, 19.94s/it]\u001b[A\n 60%|██████    | 12/20 [03:57<02:38, 19.87s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:18<02:19, 19.96s/it]\u001b[A\n 70%|███████   | 14/20 [04:37<01:59, 19.86s/it]\u001b[A\n 75%|███████▌  | 15/20 [04:57<01:38, 19.77s/it]\u001b[A\n 80%|████████  | 16/20 [05:17<01:19, 19.81s/it]\u001b[A\n 85%|████████▌ | 17/20 [05:37<00:59, 19.86s/it]\u001b[A\n 90%|█████████ | 18/20 [05:57<00:39, 19.90s/it]\u001b[A\n 95%|█████████▌| 19/20 [06:17<00:20, 20.06s/it]\u001b[A\n100%|██████████| 20/20 [06:37<00:00, 19.85s/it]\u001b[A\n 67%|██████▋   | 2/3 [12:08<06:09, 369.96s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:22<07:14, 22.84s/it]\u001b[A\n 10%|█         | 2/20 [00:46<06:58, 23.23s/it]\u001b[A\n 15%|█▌        | 3/20 [01:09<06:31, 23.04s/it]\u001b[A\n 20%|██        | 4/20 [01:32<06:07, 22.99s/it]\u001b[A\n 25%|██▌       | 5/20 [01:54<05:43, 22.88s/it]\u001b[A\n 30%|███       | 6/20 [02:17<05:20, 22.87s/it]\u001b[A\n 35%|███▌      | 7/20 [02:39<04:54, 22.66s/it]\u001b[A\n 40%|████      | 8/20 [03:02<04:32, 22.71s/it]\u001b[A\n 45%|████▌     | 9/20 [03:25<04:10, 22.81s/it]\u001b[A\n 50%|█████     | 10/20 [03:47<03:45, 22.51s/it]\u001b[A\n 55%|█████▌    | 11/20 [04:10<03:24, 22.68s/it]\u001b[A\n 60%|██████    | 12/20 [04:33<03:02, 22.80s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:56<02:39, 22.83s/it]\u001b[A\n 70%|███████   | 14/20 [05:19<02:16, 22.77s/it]\u001b[A\n 75%|███████▌  | 15/20 [05:42<01:54, 22.81s/it]\u001b[A\n 80%|████████  | 16/20 [06:05<01:31, 22.98s/it]\u001b[A\n 85%|████████▌ | 17/20 [06:29<01:10, 23.41s/it]\u001b[A\n 90%|█████████ | 18/20 [06:52<00:46, 23.28s/it]\u001b[A\n 95%|█████████▌| 19/20 [07:15<00:23, 23.22s/it]\u001b[A\n100%|██████████| 20/20 [07:39<00:00, 22.95s/it]\u001b[A\n100%|██████████| 3/3 [19:47<00:00, 395.84s/it]\n","output_type":"stream"},{"name":"stdout","text":"Final prediction...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:00<00:00, 56.23it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Applying noisy student approach with 3 iterations resulted in worse public score than for 10 iterations: 0.12450","metadata":{}},{"cell_type":"markdown","source":"Let's try:\n1. scaling features to [0, 1] range and the target with the same type of scaler and then use inverse_transform to bring back log values\n2. apply Gaussian noise with smaller sigma","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nMU, SIGMA = 0, 0.001\n\ndef scale(x_train, x_test, y_train, numeric_columns):\n    scaler = MinMaxScaler().fit(x_train[numeric_columns])\n    x_train[numeric_columns] = scaler.transform(x_train[numeric_columns])\n    x_test[numeric_columns] = scaler.transform(x_test[numeric_columns])\n    target_scaler = MinMaxScaler().fit(y_train.values.reshape(-1, 1))\n    y_train = target_scaler.transform(y_train.values.reshape(-1, 1))\n    \n    return x_train, x_test, y_train, target_scaler\n    \ndef add_noise_scale_version(x_test, pseudo_labels):\n    x_test_noise = np.random.normal(MU, SIGMA, [x_test[numeric_columns].shape[0], x_test[numeric_columns].shape[1]])\n    x_test_with_noise_only_numeric = x_test[numeric_columns].add(x_test_noise)\n    x_test_with_noise = pd.concat([x_test_with_noise_only_numeric, x_test[categorical_columns]], axis=1)\n    \n    pseudo_labels_noise = np.random.normal(MU, SIGMA, [pseudo_labels.shape[0]])\n    pseudo_labels_with_noise = pseudo_labels + pseudo_labels_noise\n    \n    return x_test_with_noise, pseudo_labels_with_noise\n\ndef combine_data_scale_version(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise):\n    x_train_combined = x_train.append(x_test_with_noise, ignore_index=True)\n    pseudo_labels_with_noise = pd.Series(data=pseudo_labels_with_noise, name='SalePrice')\n    if type(y_train) != pd.Series:\n        y_train = pd.Series(data=y_train.flatten(), name='SalePrice')\n    y_train_combined = y_train.append(pseudo_labels_with_noise, ignore_index=True)\n    return x_train_combined, y_train_combined\n\ndef student_predict_scale_version(x_test, categorical_columns, models, target_scaler):\n    y_preds = []\n    print('Final prediction...')\n    for k, model in enumerate(tqdm(models)):\n        pool_test = Pool(x_test, cat_features=categorical_columns)\n        y_preds.append(model.predict(pool_test)) \n\n    y_pred_mean = np.mean(y_preds, axis=0)\n    y_pred_inverse_scaled = target_scaler.inverse_transform(y_pred_mean.reshape(-1, 1))\n    y_pred = np.array(np.exp(y_pred_inverse_scaled).flatten())\n    \n    return y_pred\n\ndef noisy_student_scale_version(iterations):\n    x_train, y_train, x_test, numeric_columns, categorical_columns = load_and_preprocess()\n    x_train, x_test, y_train, target_scaler = scale(x_train, x_test, y_train, numeric_columns)\n    teacher_models = train_models(x_train, y_train, categorical_columns, 'Learning on initial train data...')\n    print('Iterative training...')\n    for i in tqdm(range(iterations)):\n        pseudo_labels = generate_pseudo_labels(x_test, categorical_columns, teacher_models)\n        x_test_with_noise, pseudo_labels_with_noise = add_noise_scale_version(x_test, pseudo_labels)\n        x_train, y_train = combine_data_scale_version(x_train, x_test_with_noise, y_train, pseudo_labels_with_noise)\n        student_models = train_models(x_train, y_train, categorical_columns, 'Learning on data with noise...')\n        teacher_models = student_models\n    y_pred = student_predict_scale_version(x_test, categorical_columns, teacher_models, target_scaler)\n    \n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2022-09-07T13:02:43.220653Z","iopub.execute_input":"2022-09-07T13:02:43.221132Z","iopub.status.idle":"2022-09-07T13:02:43.239699Z","shell.execute_reply.started":"2022-09-07T13:02:43.221075Z","shell.execute_reply":"2022-09-07T13:02:43.238742Z"},"trusted":true},"execution_count":99,"outputs":[]},{"cell_type":"code","source":"y_pred_3_iters_scaled = noisy_student_scale_version(3)\nsubmit_noisy_student_3_iters_scaled = pd.DataFrame()\nsubmit_noisy_student_3_iters_scaled['Id'] = test_data['Id']\nsubmit_noisy_student_3_iters_scaled['SalePrice'] = y_pred_3_iters_scaled\n\nsubmit_noisy_student_3_iters_scaled.to_csv('/kaggle/working/noisy_student_3_iters_scaled.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-09-07T13:02:46.057595Z","iopub.execute_input":"2022-09-07T13:02:46.058335Z","iopub.status.idle":"2022-09-07T13:25:20.175714Z","shell.execute_reply.started":"2022-09-07T13:02:46.058300Z","shell.execute_reply":"2022-09-07T13:25:20.174614Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Learning on initial train data...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [03:09<00:00,  9.45s/it]\n","output_type":"stream"},{"name":"stdout","text":"Iterative training...\n","output_type":"stream"},{"name":"stderr","text":"  0%|          | 0/3 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:15<05:00, 15.81s/it]\u001b[A\n 10%|█         | 2/20 [00:31<04:45, 15.87s/it]\u001b[A\n 15%|█▌        | 3/20 [00:47<04:31, 15.98s/it]\u001b[A\n 20%|██        | 4/20 [01:03<04:14, 15.88s/it]\u001b[A\n 25%|██▌       | 5/20 [01:19<04:00, 16.00s/it]\u001b[A\n 30%|███       | 6/20 [01:35<03:43, 15.94s/it]\u001b[A\n 35%|███▌      | 7/20 [01:51<03:28, 16.05s/it]\u001b[A\n 40%|████      | 8/20 [02:07<03:12, 16.06s/it]\u001b[A\n 45%|████▌     | 9/20 [02:24<02:57, 16.14s/it]\u001b[A\n 50%|█████     | 10/20 [02:40<02:40, 16.08s/it]\u001b[A\n 55%|█████▌    | 11/20 [02:56<02:25, 16.17s/it]\u001b[A\n 60%|██████    | 12/20 [03:12<02:09, 16.21s/it]\u001b[A\n 65%|██████▌   | 13/20 [03:29<01:53, 16.26s/it]\u001b[A\n 70%|███████   | 14/20 [03:44<01:36, 16.09s/it]\u001b[A\n 75%|███████▌  | 15/20 [04:00<01:20, 16.05s/it]\u001b[A\n 80%|████████  | 16/20 [04:16<01:04, 16.04s/it]\u001b[A\n 85%|████████▌ | 17/20 [04:33<00:48, 16.09s/it]\u001b[A\n 90%|█████████ | 18/20 [04:49<00:32, 16.09s/it]\u001b[A\n 95%|█████████▌| 19/20 [05:05<00:16, 16.15s/it]\u001b[A\n100%|██████████| 20/20 [05:21<00:00, 16.08s/it]\u001b[A\n 33%|███▎      | 1/3 [05:21<10:43, 321.99s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:19<06:10, 19.50s/it]\u001b[A\n 10%|█         | 2/20 [00:38<05:49, 19.41s/it]\u001b[A\n 15%|█▌        | 3/20 [00:58<05:28, 19.30s/it]\u001b[A\n 20%|██        | 4/20 [01:17<05:10, 19.43s/it]\u001b[A\n 25%|██▌       | 5/20 [01:36<04:50, 19.40s/it]\u001b[A\n 30%|███       | 6/20 [01:56<04:32, 19.43s/it]\u001b[A\n 35%|███▌      | 7/20 [02:15<04:12, 19.43s/it]\u001b[A\n 40%|████      | 8/20 [02:35<03:55, 19.59s/it]\u001b[A\n 45%|████▌     | 9/20 [02:55<03:35, 19.61s/it]\u001b[A\n 50%|█████     | 10/20 [03:14<03:15, 19.51s/it]\u001b[A\n 55%|█████▌    | 11/20 [03:34<02:56, 19.62s/it]\u001b[A\n 60%|██████    | 12/20 [03:54<02:37, 19.69s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:13<02:17, 19.62s/it]\u001b[A\n 70%|███████   | 14/20 [04:33<01:57, 19.59s/it]\u001b[A\n 75%|███████▌  | 15/20 [04:52<01:37, 19.48s/it]\u001b[A\n 80%|████████  | 16/20 [05:12<01:18, 19.50s/it]\u001b[A\n 85%|████████▌ | 17/20 [05:31<00:58, 19.57s/it]\u001b[A\n 90%|█████████ | 18/20 [05:51<00:38, 19.50s/it]\u001b[A\n 95%|█████████▌| 19/20 [06:10<00:19, 19.52s/it]\u001b[A\n100%|██████████| 20/20 [06:30<00:00, 19.51s/it]\u001b[A\n 67%|██████▋   | 2/3 [11:52<06:02, 362.34s/it]","output_type":"stream"},{"name":"stdout","text":"Learning on data with noise...\n","output_type":"stream"},{"name":"stderr","text":"\n  0%|          | 0/20 [00:00<?, ?it/s]\u001b[A\n  5%|▌         | 1/20 [00:22<07:04, 22.34s/it]\u001b[A\n 10%|█         | 2/20 [00:45<06:45, 22.56s/it]\u001b[A\n 15%|█▌        | 3/20 [01:07<06:24, 22.64s/it]\u001b[A\n 20%|██        | 4/20 [01:30<06:00, 22.50s/it]\u001b[A\n 25%|██▌       | 5/20 [01:52<05:38, 22.58s/it]\u001b[A\n 30%|███       | 6/20 [02:15<05:17, 22.65s/it]\u001b[A\n 35%|███▌      | 7/20 [02:37<04:52, 22.53s/it]\u001b[A\n 40%|████      | 8/20 [03:00<04:31, 22.62s/it]\u001b[A\n 45%|████▌     | 9/20 [03:23<04:09, 22.66s/it]\u001b[A\n 50%|█████     | 10/20 [03:45<03:45, 22.53s/it]\u001b[A\n 55%|█████▌    | 11/20 [04:07<03:21, 22.44s/it]\u001b[A\n 60%|██████    | 12/20 [04:30<02:59, 22.41s/it]\u001b[A\n 65%|██████▌   | 13/20 [04:52<02:37, 22.51s/it]\u001b[A\n 70%|███████   | 14/20 [05:15<02:14, 22.48s/it]\u001b[A\n 75%|███████▌  | 15/20 [05:37<01:52, 22.51s/it]\u001b[A\n 80%|████████  | 16/20 [06:00<01:30, 22.58s/it]\u001b[A\n 85%|████████▌ | 17/20 [06:23<01:08, 22.67s/it]\u001b[A\n 90%|█████████ | 18/20 [06:46<00:45, 22.61s/it]\u001b[A\n 95%|█████████▌| 19/20 [07:08<00:22, 22.63s/it]\u001b[A\n100%|██████████| 20/20 [07:31<00:00, 22.58s/it]\u001b[A\n100%|██████████| 3/3 [19:24<00:00, 388.19s/it]\n","output_type":"stream"},{"name":"stdout","text":"Final prediction...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 20/20 [00:00<00:00, 58.85it/s]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Previous best public score was: 0.12102\n\nThis time, with applied scaling, noisy student helped to achieve the new best score: 0.12032\n","metadata":{}}]}